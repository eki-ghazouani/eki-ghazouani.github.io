"use strict";(self.webpackChunkeki_ghazouani_github_io=self.webpackChunkeki_ghazouani_github_io||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"/2022/09/20/newsletter_Sept-2022","metadata":{"permalink":"/blog/2022/09/20/newsletter_Sept-2022","source":"@site/blog/2022-09-20-newsletter_Sept-2022.md","title":"Newsletter for September 2022","description":"Hi everyone, We are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!","date":"2022-09-20T00:00:00.000Z","formattedDate":"September 20, 2022","tags":[{"label":"Data Science","permalink":"/blog/tags/data-science"},{"label":"Data Engineering","permalink":"/blog/tags/data-engineering"},{"label":"Data Mesh","permalink":"/blog/tags/data-mesh"},{"label":"NPM","permalink":"/blog/tags/npm"},{"label":"Hopular","permalink":"/blog/tags/hopular"}],"readingTime":4.26,"hasTruncateMarker":true,"authors":[{"name":"Ali Ghazouani","title":"Junior Data Scientist Consultant","url":"mailto:inno@ekimetrics.com","imageURL":"https://github.com/eki-ghazouani.png","key":"ali.ghazouani"}],"frontMatter":{"title":"Newsletter for September 2022","header_image_url":"./img/blog/header.png","authors":["ali.ghazouani"],"tags":["Data Science","Data Engineering","Data Mesh","NPM","Hopular"],"draft":false,"description":"Hi everyone, We are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!","keywords":["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Engineering","App and Web Development","Data Science for business"]},"nextItem":{"title":"Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach","permalink":"/blog/2022/09/06/deep_rl"}},"content":"import head from \'@docusaurus/Head\';\\r\\n\\r\\n<head>\\r\\n    <meta property=\\"og:image\\" content=\\"./img/blog/header.png\\" />\\r\\n</head>\\r\\n\\r\\n\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nHi everyone, we are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!\\r\\n </div>\\r\\n\\r\\n\\r\\n\\r\\n## Data Science \\r\\n\\r\\n### Why You Should Warn Customers When You\u2019re Running Low on Stock   \\r\\n\\r\\n![](img/newsletter_september_2022/DS_article_1.jpg)\\r\\n\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nThe supply-chain disruptions due to the pandemic and the Ukraine war caused the retailers to face unprecedented stockouts risks. To overcome this challenge, Instacart suggest that honesty is the best policy. By using a Machine Learning model to predict that an item is likely out-of-stock and therefore warning clients when it\u2019s the case, they reduced the proportion of replacements and refunds in the short term and increased the total revenue per customer in the long term. \\r\\n </div>\\r\\n\\r\\n <p>&nbsp;</p>\\r\\n\\r\\n[Why You Should Warn Customers When You\u2019re Running Low on Stock - Harvard Business Review](https://hbr.org/2022/09/why-you-should-warn-customers-when-youre-running-low-on-stock)\\r\\n\\r\\n\\r\\n## Machine Learning\\r\\n\\r\\n### Hopular: Modern Hopfield Networks for Tabular Data\\r\\n\\r\\n![](img/newsletter_september_2022/ML_DL.jpg)\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nWhile Deep Learning excels in structured data as encountered in vision and natural language processing, it failed to meet its expectations on tabular data. For tabular data, Support Vector Machines (SVMs), Random Forests, and Gradient Boosting are the best performing techniques with Gradient Boosting in the lead. \\r\\n </div>\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\n\\"Hopular\\" is a novel Deep Learning architecture for medium- and small-sized datasets, where each layer is equipped with continuous modern Hopfield networks.\\r\\n </div>\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nIn experiments on small-sized tabular datasets with less than 1,000 samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in particular several Deep Learning methods. In experiments on medium-sized tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost, LightGBM and a state-of-the art Deep Learning method designed for tabular data. Thus, Hopular is a strong alternative to these methods on tabular data.\\r\\n </div>\\r\\n\\r\\n<p>&nbsp;</p>\\r\\n\\r\\n\\r\\n[GitHub - Hopular](https://ml-jku.github.io/hopular/)\\r\\n\\r\\n## Data Engineering & Architecture\\r\\n\\r\\n\\r\\n### How to learn data engineering\\r\\n\\r\\n![](img/newsletter_september_2022/DE_article_1.jpg)\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nLike data scientists, data engineers write code. They\u2019re highly analytical, and are interested in data visualization. Unlike data scientists \u2014 and inspired by our more mature parent,\xa0software engineering\xa0\u2014 data engineers build tools, infrastructure, frameworks, and services. In fact, it\u2019s arguable that data engineering is much closer to software engineering than it is to a data science.\\r\\n </div>\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nThis post introduces the role of a data engineer and the challenges he faces on a daily basis. It also provides some very interesting links to acquire the basics or to consolidate your knowledge in this domain.\\r\\n </div>\\r\\n\\r\\n<p>&nbsp;</p>\\r\\n\\r\\n[How to learn data engineering | Blef.fr](https://www.blef.fr/learn-data-engineering/)\\r\\n<p>&nbsp;</p>\\r\\n\\r\\n### Do\u2019s and Don\u2019ts of Data Mesh\\r\\n\\r\\n![](img/newsletter_september_2022/DE_article_2.jpg)\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nMany enterprises are investing in their next generation data lake, with the hope of democratizing data at scale to provide business insights and ultimately make automated intelligent decisions. Data platforms based on the data lake architecture have common failure modes that lead to unfulfilled promises at scale. To address these failure modes, a new paradigm saw the light : Data Mesh. \\r\\n </div>\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nThis article introduces Data Mesh and offers a series of advices, Do\u2019s and Don\u2019ts as a result of its implementation in BlaBlaCar. \\r\\n </div>\\r\\n<p>&nbsp;</p>\\r\\n\\r\\n[Do\u2019s and Don\u2019ts of Data Mesh | Kineret Kimhi | Medium](https://medium.com/blablacar/dos-and-don-ts-of-data-mesh-e093f1662c2d)\\r\\n\\r\\n\\r\\n## App and Web Development\\r\\n\\r\\n### Introducing Signals\\r\\n\\r\\n![](img/newsletter_september_2022/App_article_1.jpg)\\r\\n\\r\\nSignals are a way of expressing state that ensure apps stay fast regardless of how complex they get. Signals are based on reactive principles and provide excellent developer ergonomics, with a unique implementation optimized for Virtual DOM.\\r\\n\\r\\n[Introducing Signals | Preactjs.com](https://preactjs.com/blog/introducing-signals/)\\r\\n\\r\\n### Best practices for creating a modern npm package\\r\\n\\r\\n![](img/newsletter_september_2022/App_article_2.jpg)\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nNPM is an online repository for the publishing of open-source Node.js projects, it also allows to interact with the repository through a command-line utility for package installation, version management and dependency management. \\r\\n </div>\\r\\n\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nThis article details the best practices for creating and managing an npm package such as security checks, automated semantic version management etc.\\r\\n </div>\\r\\n<p>&nbsp;</p>\\r\\n\\r\\n[Best practices for creating a modern npm package | Brian Clark | Synk.io](hhttps://snyk.io/blog/best-practices-create-modern-npm-package/)\\r\\n\\r\\n## Special Section: Responsible AI \\r\\n\\r\\n### OpenRAIL: Towards open and responsible AI licensing frameworks\\r\\n\\r\\n![](img/newsletter_september_2022/responsible.jpg)\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nAdvances in machine learning and other AI-related areas have flourished these past years partly thanks to thethe open source culture. It has in fact allowed knowledge sharing and created communities that fostered innovation. Nevertheless, recent events related to the ethical and socio-economic concerns of development and use of machine learning models have spread a clear message: Making sure AI is responsible is incompatible with open-source. Yet, closed systems are not the answer, as the problem persists under the opacity of firms\' private AI development processes. \\r\\n </div>\\r\\n\\r\\n<div align=\\"justify\\"> \\r\\nIn this context, the OpenRAIL approach suggests a new type of licensing that embed a specific set of restrictions to make sure of the good usage of the models. Therefore, while benefiting from an open access to the ML model, the user will not be able to use the model for the specified restricted scenarios.\\r\\n </div>\\r\\n\\r\\n\\r\\n<p>&nbsp;</p>\\r\\n\\r\\n\\r\\n[OpenRAIL: Towards open and responsible AI licensing frameworks | Carlos Munoz Ferrandis | huggingface.co](https://huggingface.co/blog/open_rail)\\r\\n\\r\\n\\r\\n\\r\\n## Credits\\r\\n- Photos from <a href=\\"https://unsplash.com\\">Unsplash</a>"},{"id":"/2022/09/06/deep_rl","metadata":{"permalink":"/blog/2022/09/06/deep_rl","source":"@site/blog/2022-09-06-deep_rl.md","title":"Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach","description":"This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on an approach based on two famous reinforcement learning algorithms: Q-Learning and Policy Gradient.","date":"2022-09-06T00:00:00.000Z","formattedDate":"September 6, 2022","tags":[{"label":"Operational Research","permalink":"/blog/tags/operational-research"},{"label":"Optimization","permalink":"/blog/tags/optimization"},{"label":"Knapsack problem","permalink":"/blog/tags/knapsack-problem"},{"label":"Deep Reinforcement Learning","permalink":"/blog/tags/deep-reinforcement-learning"}],"readingTime":13.53,"hasTruncateMarker":true,"authors":[{"name":"Nathan Rouff","title":"Data Scientist Consultant","url":"mailto:inno@ekimetrics.com","imageURL":"/img/authors/nathan_rouff.png","key":"nathan.rouff"}],"frontMatter":{"title":"Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach","authors":["nathan.rouff"],"header_image_url":"./img/blog/slovenia_bled_lake.jpg","tags":["Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"],"draft":false,"description":"This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on an approach based on two famous reinforcement learning algorithms: Q-Learning and Policy Gradient.","keywords":["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem","Deep Reinforcement Learning"]},"prevItem":{"title":"Newsletter for September 2022","permalink":"/blog/2022/09/20/newsletter_Sept-2022"},"nextItem":{"title":"Deep RL and Optimization applied to Operations Research problem - 1/2 Traditional Optimization techniques","permalink":"/blog/2022/08/27/traditional_or"}},"content":"\x3c!-- import useBaseUrl from \\"@docusaurus/useBaseUrl\\";\\r\\n\\r\\n<link rel=\\"stylesheet\\" href=\\"{useBaseUrl(\'katex/katex.min.css\')}\\" />\\r\\n --\x3e\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n\\r\\n\\r\\nThis article is the second part of the serie of articles introducing optimization techniques for solving the classical Operations Research problem of multi-knapsack. The main objective of this article is to introduce Reinforcement Learning as a way to solve combinatorial optimization problems (Reinforcement Learning can actually be used to solve a much wider range of optimization problems). \\r\\n\\r\\nFirst, the classical Reinforcement Learning framework will be briefly presented. Then, we\'ll see how to frame the multi-knapsack problem for Reinforcement Learning, followed by explanations on why we chose to explore RL for this combinatorial optimization problem. Eventually, the Q-learning (no neural networks) and Policy Gradient (with neural networks) approaches will be introduced and their performance will be evaluated on the knapsack problem.\\r\\n\\r\\n\\r\\n## Reinforcement Learning for the Knapsack problem \\r\\n\\r\\n### What is Reinforcement Learning?\\r\\n\\r\\nThe image below represents the Reinforcement Learning framework. It describes in a simple, yet accurate manner, one of the main ideas behind Reinforcement Learning. \\r\\n\\r\\n\\r\\n![screenshot-app](img/RL_images/reinforcement-learning-framework.jpg)\\r\\n\\r\\n<div align=\\"center\\"> Figure 1 : The Reinforcement Learning framework\\r\\n\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\n\\r\\nBasically, an agent receives information about the state of an environment he evolves in, information we will call S<sub>t</sub> as it describes the state at timestep t.\\r\\n\\r\\nBased on this information it receives, the RL agent will choose an action among all the actions it has the right to take at each timestep. We will call such action A<sub>t</sub>, the action at time t, with A<sub>t</sub> belonging to AA<sub>t</sub>(s<sub>t</sub>) the set of available actions given the state S<sub>t</sub>. When an action is taken, it has an impact on the environment and the agent will receive information about the new state of the environment S<sub>t+1</sub> but also a reward to incentivize it to take actions which will maximize the total rewards it expects to obtain at the end of an episode.\\r\\n\\r\\nTo apply reinforcement learning to solve business problems, these problems have to be framed as a Markov Decision Process, as seen above. More details can be found on how to rigorously define the Reinforcement Learning in the [excellent course given by David Silver](https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver) (principal research scientist at DeepMind, now owned by Google). You may find his lectures using the previous link, with lectures 1 and 2 being the most pertinent.\\r\\n\\r\\nNow, to get a better grasp on how to frame a problem for Reinforcement Learning, let\u2019s consider two practical examples.\\r\\n\\r\\nAs a first example, we can consider for instance an AI trader. It could have as available actions the possibility to buy or sell many different products. Its actions have an impact on the environment. First, the money it has and the products it owns will be modified, but also if it buys a massive amount of a certain product, it may have an important impact on the future prices. The final goal for it may be to earn as much money as possible. The description of this first example with the prism of Reinforcement Learning is given in figure 2.\\r\\n\\r\\n![screenshot-app](img/RL_images/Example_AI_Trader_no_logo.png)\\r\\n<div align=\\"center\\"> Figure 2 - Example of an AI trader described through the prism of Reinforcement Learning\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\n\\r\\nFor the case of a self driving car as the AI agent, the actions it can take could be turning, stopping, accelerating. The information it will receive at each timestep are the speed of the car, its geolocation and probably many others. The environment can be the real world around the car, or just a simulator. The final reward will take into account how fast the car has reached a certain goal position, without damaging things or killing people for instance. Should it damage objects, it could for instance receive negative rewards. This information is summarized in the figure 3 below.\\r\\n\\r\\n![screenshot-app](img/RL_images/Example_self_driving_car_no_logo.png)\\r\\n<div align=\\"center\\"> Figure 3 - Example of a self driving car described through the prism of Reinforcement Learning\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\n\\r\\nLet\u2019s now tackle the case of the multi-knapsack problem!\\r\\n\\r\\n### How to adapt the multi-Knapsack problem for solving with Reinforcement Learning\\r\\n\\r\\nThe precise definition of the multi-knapsack problem was given in the first part of this serie of articles on the knapsack problem. The figure below describes visually the problem at stake.\\r\\n\\r\\n![screenshot-app](img/RL_images/Knapsack_problem_5.png)\\r\\n<div align=\\"center\\"> Figure 4 - Description of the multi-knapsack problem\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\nIn our case, one could think about the agent as a person trying to carefully choose among the many clothes he/she possesses before going on a long trip. The environment would be the empty bags and all the clothes to choose from. At each timestep, the person would have the choice to take one element among the available clothes to put it inside one of the bags, the bags needing to be closed (and thus not to full) before leaving for the trip.\\r\\n\\r\\nThe objective is to maximize the value of the clothes chosen for the trip.\\r\\n\\r\\n\\r\\nAnd that\u2019s it! Our problem is framed for Reinforcement Learning.\\r\\n\\r\\n\\r\\n### Why we chose to explore RL for combinatorial optimization problems\\r\\nThe last two decades have known the breakthrough of Deep Learning which is now massively entering all fields of industry whether this is for Computer Vision, disease predictions, product recommendation, Natural Language Processing applications, etc. Massive investments follow in the field of Machine Learning implying a virtuous circle with more results and regular new breakthroughs. Due to these developments, Deep Reinforcement Learning has emerged from the field of Reinforcement Learning which has been studied for a long time and whose goal is to take actions in an environment in order to maximize a defined cumulative reward. This allowed new recent breakthroughs, such as the AI AlphaGo beating professional Go players in 2016 and more recently AlphaStar beating world champions of the video game Starcraft (more on that in [DeepMind blog article](https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii) or in the [Nature paper](https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D)). \\r\\n\\r\\n![screenshot-app](img/RL_images/AlphaStar_Image.png)\\r\\n<div align=\\"center\\"> Figure 5 - Visualization highlighting the trained AlphaStar performing against a top Starcraft human players\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\nWe\u2019ll say a few words about this video game environment, because this achievement is one of the reasons which motivates the use of Reinforcement Learning for solving combinatorial optimization problems. Indeed, with the knapsack problem, we have a discrete action space with a limited number of actions, although the range of available actions can become extremely high by changing the number of available items and knapsacks. \\r\\n\\r\\nHaving an AI agent beating the world\u2019s best players on this game is an important breakthrough as this video game environment is extremely complex, with only imperfect information being provided to the agent, the action space being enormous with a choice between up to 10<sup>26</sup> different actions, and actions being taken almost in real time, every 0.2 seconds. Eventually, planning is made on long term and the agent doesn\u2019t know until the end of the game whether it has won the game or not. While applying Deep Reinforcement Learning to video games allows to test the performance of the algorithms very accurately, allowing to judge how it performs in very different environments, applications also begin to appear in other fields, opening the perspective of using these techniques in different industry fields in the next few years. Deep Reinforcement Learning is definitely a field with high potential, and proofs have been shown that it can solve very well high-dimensional problems. Especially, many articles were published where these algorithms were applied to finance problems.\\r\\n\\r\\nNow that we have seen the potential of Reinforcement Learning for solving a problem such as the knapsack problem, it is important to keep in mind some important characteristics of Deep Reinforcement Learning approaches: \\r\\n\\r\\n- Reinforcement Learning algorithms provides us with approximations of the optimal solutions on the contrary to the solutions that could provide Mixed Integer Programming solvers as the ones introduced in the first part of this series of articles on the knapsack problem;\\r\\n- For the same reason, Reinforcement Learning algorithms will always provide us solutions to the problem, on the contrary to exact methods which could be unable to provide any solution for very complex problems. For that reason, Reinforcement Learning approaches are for instance being developed in order to solve partial differential equations of very high dimensionality, where usual solvers are unable to provide a solution;\\r\\n- Reinforcement Learning algorithms perform online optimization, meaning that once they have been trained, they are able to solve very complex problems immediately. They have thus tremendous potential for applications which require to solve problems very frequently in a limited time window, such as in trading or product recommendation for instance.\\r\\n\\r\\nAs seen in the beginning of this section, Reinforcement Learning algorithms have a very high potential for a wide range of business problems. Let\'s now introduce one of the two Reinforcement Learning approaches used in this notebook. The first one, the Q-learning approach, isn\'t based on neural networks and doesn\'t scale well when the dimensionality of the problem increases. We have studied it as it is at the core of other important algorithms such as Deep Q Learning, much more powerful. We will thus concentrate on another promising approach, based on neural networks: the Policy Gradient approach.\\r\\n\\r\\n\\r\\n\\r\\n## Using Q-Learning and Policy Gradient algorithms on the Knapsack problem\\r\\n\\r\\n### A simple introduction to Policy Gradient \\r\\nThe basic principle with a policy gradient approach is that, for each state _s_ received as input, our algorithm will provide us a probability distribution for the actions to take, allowing us to know which object our algorithm recommands us to put first in the knapsack.\\r\\n\\r\\nIn the formula below, \ud835\udf0b gives us this probability. More precisely, \ud835\udf0b gives us the probability to take action _a_ knowing that we currently at state _s_ and given the \ud835\udf03 values of the model parameters, neurons in our case as we use neural networks. \\r\\n\\r\\n![screenshot-app](img/RL_images/policy_gradient_equation_2.svg)\\r\\n\\r\\n<div align=\\"center\\"> \\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\nThe use of neural networks isn\'t mandatory here, but very frequent to obtain good results on complex problems. An example of a representation of a simple neural network is given recalled below.  \\r\\n\\r\\n![screenshot-app](img/RL_images/classical_neural_network_image_2.png)\\r\\n<div align=\\"center\\"> Figure 6 - Representation of a simple neural network\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\nOur model architecture can be visualized with this representation. If we dig a bit deeper into the details of our model\'s architecture, we have:\\r\\n\\r\\n- As input of the neural network, a description of the current state of the system, that is the value and price information for all items, information about the already selected items and about the current and maximum weight limit inside the different bags;\\r\\n\\r\\n- Thanks to this information, our model associates to each possible action a probability and we can then select the action that the algorithm recommends us to take first, that is which object should be stored in which knapsack at the current timestep. This is the output of our model;\\r\\n\\r\\n- The parameters of this model, the neurons, are updated at the end of each episode, an episode beginning when all the objects are available and ending when the bags are sufficiently full (or all objects have been selected...). The update of the parameters (\ud835\udf03) of the model taking place at the end of each episode only and not each time an action is proposed by the model, the approach is called a __Monte Carlo approach__;\\r\\n\\r\\n- The updates of the parameters are made in order to maximize the value of the items stored inside the knapsacks and this approach is based on techniques such as __stochastic gradient descent__.\\r\\n\\r\\nNow that the Policy Gradient has been described, let\'s see how our algorithms performed on the knapsack problem!\\r\\n\\r\\n\\r\\n\\r\\n### Evaluating the performance of the RL algorithms\\r\\n\\r\\nAs summarized in figure 7 below, in order to evaluate the performance of the different algorithms, we chose to apply our two RL algorithms (Q-Learning and Policy Gradient) to 3 different environments of increasing difficulty. We trained each algorithm over 400 episodes.\\r\\n\\r\\nAt the beginning of a new experience, the algorithm had all its coefficients reinitialized. We perform several experiences in order to evaluate how robust is the algorithm. \\r\\n\\r\\n![screenshot-app](img/RL_images/evaluation_rl_algos_2.png)\\r\\n<div align=\\"center\\"> Figure 7 - Description of the evaluation process for the different algorithms\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\n\\r\\nEventually, we evaluated the performance of the algorithms with 3 metrics:\\r\\n- The mean reward shows how good on average the algorithm is;\\r\\n- The standard deviation highlights the potential lack of robustness of the algorithm;\\r\\n- The performance ratio RL vs MILP tells us how close is the RL algorithm to the optimal solution provided by a MILP solver (details on how to obtain such a solution are given in the notebook). \\r\\n\\r\\n\\r\\n\\r\\n### Results with Q-Learning (no neural networks)\\r\\n\\r\\nThe graphs on the left show that overall our Q-Learning algorithm does indeed improve through training as its reward improved over time. However, we can see that the performance ratio RL vs MILP is very low, meaning that it is far from achieving as good results as what we could get using state-of-the-art MILP solvers.\\r\\n\\r\\n![screenshot-app](img/RL_images/performance_q_learning_knapsack_4.png)\\r\\n<div align=\\"center\\"> Figure 8 - Performance of the Q-Learning algorithm\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\nFurthermore, environments of increasing complexity / dimensionality will be much more difficult to handle for Q-Learning, as its Q-value matrix has as number of columns the number of items multiplied by the number of knapsacks and as rows all the possible states which could exist. Increasing only slightly the number of knapsacks or bags will thus quickly make the Q-Learning algorithm unusable.\\r\\n\\r\\n### Results with Policy Gradient (based on neural networks)\\r\\n\\r\\nThe results of the learning process with Policy Gradient are much better than with Q-learning. At the beginning, due to random initialization of the neural network parameters, the actions are taken at random and the reward is very low, but it quickly improves until reaching a local maximum, not global as it is still lower than the solution obtained with the MILP solver. \\r\\n\\r\\nThe performance ratio is quite good on the three different environments, reaching approximately 80% for each. The algorithm scales well when the complexity increases.\\r\\n\\r\\nWe see however that the standard deviation is quite high, which highlights the fact that each time a model is initialized, it can converge to quite different values. It is thus not extremely robust and several initializations are required before finding good results approaching the optimal solution.\\r\\n\\r\\n![screenshot-app](img/RL_images/performance_policy_gradient_knapsack_2.png)\\r\\n<div align=\\"center\\"> Figure 9 - Performance of the REINFORCE (policy-gradient approach) algorithm\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\nOn the graph below are highlighted some of the limitations we have witnessed with Policy Gradient algorithms such as the REINFORCE algorithm. We have a lack of robustness, having our algorithm sometimes working very well, sometimes leading to a poorer reward.\\r\\n\\r\\n![screenshot-app](img/RL_images/robustness_pb_policy_gradient_2.png)\\r\\n<div align=\\"center\\"> Figure 10 - REINFORCE algorithm (gradient-policy approach) appears as lacking robustness\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\nFor that reason, the hyperparameter tuning is made more complicated. Indeed comparing one combination of hyperparameters with another one isn\'t enough to be certain about which combination of hyperparameters is the best, because of the high variability of results for fixed hyperparameters. \\r\\n\\r\\n## Opening\\r\\n\\r\\nFor obtaining the results given in this article, we reproduced classical Reinforcement Learning algorithms: the Q-learning algorithm which does not rely on the use of neural networks, and a Policy Gradient algorithm which relies on neural networks. We saw that the latter obtained much better results than the former as we could expect. \\r\\n\\r\\nWe also built our own multi-knapsack environment file, allowing us to easily modify greatly the complexity of the environment by increasing both the number of available items to put in the knapsacks and the number of knapsacks. \\r\\n\\r\\nWhile creating the environment file, we followed the nomenclature proposed by Open AI Gym for building Reinforcement Learning environments, using the same method names used to define an Open AI Gym environment. The objective was to be able to experiment much more quickly in the future by making use of one of the different Deep Reinforcement Learning libraries (Stable Baselines, TF Agents, Tensorforce\u2026). Indeed, these libraries allow access to many different advanced Deep Reinforcement Learning algorithms already implemented, which can directly be used on new problems if the environment file describing the problem has been built using Open AI Gym nomenclature.\\r\\n\\r\\nAnother article will be written soon to tell more about how to perform hyperparameter tuning for RL using the hyperparameter optimization framework [Optuna](https://optuna.org/) and how to compare and evaluate the efficiency of many different RL algorithms using [Stable Baselines](https://stable-baselines3.readthedocs.io/en/master/)!"},{"id":"/2022/08/27/traditional_or","metadata":{"permalink":"/blog/2022/08/27/traditional_or","source":"@site/blog/2022-08-27-traditional_or.md","title":"Deep RL and Optimization applied to Operations Research problem - 1/2 Traditional Optimization techniques","description":"This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on traditional optimization techniques.","date":"2022-08-27T00:00:00.000Z","formattedDate":"August 27, 2022","tags":[{"label":"Operational Research","permalink":"/blog/tags/operational-research"},{"label":"Optimization","permalink":"/blog/tags/optimization"},{"label":"Knapsack problem","permalink":"/blog/tags/knapsack-problem"},{"label":"Solvers","permalink":"/blog/tags/solvers"}],"readingTime":6.065,"hasTruncateMarker":true,"authors":[{"name":"Nathan Rouff","title":"Data Scientist Consultant","url":"mailto:inno@ekimetrics.com","imageURL":"/img/authors/nathan_rouff.png","key":"nathan.rouff"}],"frontMatter":{"title":"Deep RL and Optimization applied to Operations Research problem - 1/2 Traditional Optimization techniques","authors":["nathan.rouff"],"header_image_url":"./img/blog/plitvice_lakes.jpg","tags":["Operational Research","Optimization","Knapsack problem","Solvers"],"draft":false,"description":"This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on traditional optimization techniques.","keywords":["Data Science","EkiLab","Ekimetrics","Eki.Lab","Eki","Machine Learning","Artificial Intelligence","Data Science for business","Operational Research","Optimization","Knapsack problem"]},"prevItem":{"title":"Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach","permalink":"/blog/2022/09/06/deep_rl"}},"content":"\x3c!-- import useBaseUrl from \\"@docusaurus/useBaseUrl\\";\\r\\n\\r\\n<link rel=\\"stylesheet\\" href=\\"{useBaseUrl(\'katex/katex.min.css\')}\\" />\\r\\n --\x3e\\r\\n\x3c!--truncate--\x3e\\r\\n\\r\\n\\r\\nimport Tabs from \'@theme/Tabs\';\\r\\nimport TabItem from \'@theme/TabItem\';\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLet $f\\\\colon[a,b]\\\\to\\\\R$ be Riemann integrable. Let $F\\\\colon[a,b]\\\\to\\\\R$ be\\r\\n$F(x)=\\\\int_{a}^{x} f(t)\\\\,dt$. Then $F$ is continuous, and at all $x$ such that\\r\\n$f$ is continuous at $x$, $F$ is differentiable at $x$ with $F\'(x)=f(x)$.\\r\\n\\r\\n$$\\r\\nI = \\\\int_0^{2\\\\pi} \\\\sin(x)\\\\,dx\\r\\n$$\\r\\n\\r\\nIn this first article is introduced a systematic way to approach and solve optimization problems. Then, the multi-knapsack problem itself is introduced. Then we apply the rules defined before on how to solve optimization problems and obtain the optimal solution to the multi-knapsack problem, formulated as a Mixed Integer problem and using Python-MIP package. Let\'s now introduce simple steps one can follow to approach optimization problems with optimization solvers.\\r\\n\\r\\n\\r\\n## Main steps while creating an optimization model to solve a business problem\\r\\n\\r\\nOnce a business problem that could benefit from optimization has been identified, we can define a systematic approach based on 3 steps for solving all kind of optimization problems with optimization solvers. These 3 steps are highlighted in the figure below.\\r\\n\\r\\n![screenshot-app](img/RL_images/3_steps_math_modelling_4.png)\\r\\n\\r\\n<div align=\\"center\\"> Figure 1 : The 3 main steps for solving a business problem through optimization\\r\\n\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\n\\r\\n\\r\\nIn more details, these 3 steps are: \\r\\n\\r\\n1. __Create the conceptual mathematical model__ that defines the different variables, constraints, etc. in the business problem. This step consists in writing down on paper the equations that define our problem. \\r\\n\\r\\n2. __Translate the conceptual mathematical model into a computer program__. For most programming languages used for optimization, the computer program will largely resembles the mathematical equations one would write on paper.\\r\\n\\r\\n3. __Solve the mathematical model using a math programming solver__. The solver available for Mathematical Programming (solvers such as GLPK, Gurobi, CPLEX...) relies on very sophisticated algorithms. Important algorithms and ideas used in these solvers are, among many others: simplex method, branch & bound, use of heuristics...\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nLet\'s see those 3 steps for the case of the multi-knapsack problem.\\r\\n\\r\\n\\r\\n## The multi-knapsack problem\\r\\n\\r\\n\\r\\nThe objective here is, given a set of _n_ items and a set of _m_ knapsacks, to __maximize__ the total value of the items put in the knapsacks without exceeding their capacity.\\r\\n\\r\\nBelow,  w<sub>i</sub> represents the weight of item i,  p<sub>i</sub> the value of item i while  c<sub>j</sub> represents the capacity of knapsack j.\\r\\n\\r\\n![screenshot-app](img/RL_images/Knapsack_problem_5.png)\\r\\n\\r\\n<div align=\\"center\\"> Figure 2: Description of the multi-knapsack problem\\r\\n\\r\\n\\r\\n </div>\\r\\n<br/>\\r\\n\\r\\n\\r\\n\\r\\nThe multi-knapsack is an extension of the classical knapsack problem where instead of considering only one knapsack, we consider as many as we want. This allows to easily extend the complexity of this problem.\\r\\n\\r\\nWhile the problem is relatively easy to define mathematically, it belongs to the class of NP-hard problems. Without going into the details of what defines NP-hard problems, we can easily see that the complexity of the knapsack problems explodes when the number of knapsacks and items increases. Indeed, we have m<sup>n</sup> available combinations we would need to test should we want to apply a brute-force approach for solving this problem. Just with 10 knapsacks and 80 items, there are 10<sup>80</sup> combinations, which is the estimation of the number of atoms in the universe! And 10 knapsacks and 80 items is still quite limited... Let\'s now try to create the conceptual mathematical model by defining the problem with equations.\\r\\n\\r\\n\\r\\n### Creating the conceptual mathematical model\\r\\n\\r\\nA quick translation of the multi-knapsack problem with equation can be written as the following: \\r\\n\\r\\n![screenshot-app](img/RL_images/equations_1.svg)\\r\\n![screenshot-app](img/RL_images/equations_3.svg)\\r\\n\\r\\n\\r\\n\\r\\nNow that we managed to translate the problem into a set of equations, let\'s translate this mathematical model so that it is understood by a computer program. Below, we will make use of the Python package [Python-MIP](https://www.python-mip.com/) which is open-source and provides tools for modeling and solving Mixed-Integer Linear Programming Problems (MIP), relying on fast open source solvers.\\r\\n\\r\\n### Translating the mathematical model into a computer program with Python-MIP\\r\\n\\r\\nBefore solving the problem, we have to generate an instance for it (have data defining the problem). To do so, you can use the following code that will generate an instance of this problem with 40 items to store in 5 bags.\\r\\n\\r\\n```\\r\\nimport pandas as pd\\r\\nimport numpy as np\\r\\nimport pickle\\r\\n\\r\\ndef data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag):\\r\\n    data = {}\\r\\n    weights = np.random.randint(minimum_weight_item, maximum_weight_item, size = number_items)\\r\\n    values = np.random.randint(minimum_value_item, maximum_value_item, size = number_items)\\r\\n    data[\'weights\'] = weights\\r\\n    data[\'values\'] = values\\r\\n    data[\'items\'] = list(range(len(weights)))\\r\\n    data[\'num_items\'] = len(weights)\\r\\n    data[\'bins\'] = list(range(number_bags))\\r\\n    data[\'bin_capacities\'] = np.random.randint(0, max_weight_bag, size = number_bags) + np.int(np.mean(data[\'weights\']))\\r\\n    return(data)\\r\\n\\r\\nnumber_bags = 5\\r\\nnumber_items = 40\\r\\nminimum_weight_item = 0\\r\\nmaximum_weight_item = 75\\r\\nminimum_value_item = 0\\r\\nmaximum_value_item = 75\\r\\nmax_weight_bag = 150\\r\\n\\r\\ndata = data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag)\\r\\n```\\r\\n\\r\\n\\r\\n\\r\\nLet\'s now import the package used to have access to the MIP solver, here using the python package Python-MIP:\\r\\n\\r\\n```\\r\\nfrom mip import Model, xsum, maximize, BINARY\\r\\n```\\r\\n\\r\\nNow, we can translate the mathematical model so that it is understood by Python-MIP. \\r\\n\\r\\n```\\r\\ndef mip_solve_knapsack(data):\\r\\n\\r\\n  model = Model(\\"knapsack\\")\\r\\n\\r\\n  x = [[model.add_var(var_type=BINARY) for i in data[\'items\']] for j in data[\'bins\']]\\r\\n\\r\\n  model.objective = maximize(xsum((xsum(data[\'values\'][i] * x[j][i] for i in data[\'items\']) for j in data[\'bins\'])))\\r\\n\\r\\n  for j in data[\'bins\']:\\r\\n      model += xsum(data[\'weights\'][i] * x[j][i] for i in data[\'items\']) <= data[\'bin_capacities\'][j]\\r\\n\\r\\n  # Each item can be in at most one bin\\r\\n  for i in data[\'items\']:\\r\\n      model += xsum(x[j][i] for j in data[\'bins\']) <= 1\\r\\n\\r\\n  model.optimize()\\r\\n  \\r\\n  return(model)\\r\\n```\\r\\n\\r\\nRemark how close it is from the original equations! These solvers are very powerful and yet easy to use directly in Python. The code is indeed very close to the original equations. \\r\\n\\r\\n\\r\\n\\r\\n\\r\\n### Solving the mathematical model with Python-MIP\\r\\n\\r\\nUsing the **mip_solve_knapsack** function defined in the previous section, we can access to important information regarding the problem, such as the final objective value and the values of x<sub>ij</sub> telling us what were the best combinations of items inside knapsacks.\\r\\n\\r\\n\\r\\n### Some Mathematical Optimization packages\\r\\n\\r\\nIn the notebook associated to this article, the package Python-MIP was used. __Python-MIP__ is free, but many other packages exist for solving optimization problems on Python (and other languages of course like Julia). For instance __OR-Tools__ from Google is a well-recognized free solver, with [detailed documentation](https://developers.google.com/optimization/introduction/overview). \\r\\n\\r\\nOn the other side, __Gurobi__ is a very popular commercial solution for mathematical optimization and its documentation is extremely rich, with quick introductions about [Mathematical Programming](https://www.gurobi.com/resource/modeling-basics/), [Linear Programming](https://www.gurobi.com/resource/mip-basics/) and [Mixed-Integer Programming](https://www.gurobi.com/resource/mip-basics/). Importantly, it has a [large number of modeling examples from all industry fields](https://www.gurobi.com/resource/modeling-examples-using-the-gurobi-python-api-in-jupyter-notebook/) directly available on Google Colab allowing to better grasp notions of Mathematical Modelling and to improve modeling skills to tackle all kind of optimization problems with Python. This resource can be of use even if one doesn\'t plan to use this commercial software but rather a free package such as OR-Tools.\\r\\n\\r\\n## Conclusion\\r\\n\\r\\nIn this article was introduced the multi-knapsack problem, an NP-complete problem, very difficult to solve when taking many items and bags. \\r\\n\\r\\nThe approach to solve the multi-knapsack problem relied on Python-MIP, a free optimization package using powerful MILP solvers to solve very efficiently all kinds of optimization problems.\\r\\n\\r\\nIn the next part of this series on the multi-knapsack problem, well studied in the field of Operations Research and at the heart of many real optimization problems, we\'ll highlight how Deep Reinforcement Learning can be used in order to solve combinatorial optimization problems such as this one. Stay tuned!"}]}')}}]);