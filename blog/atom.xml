<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://eki-ghazouani.github.io/blog</id>
    <title>Eki.Lab Blog</title>
    <updated>2022-10-26T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://eki-ghazouani.github.io/blog"/>
    <subtitle>Eki.Lab Blog</subtitle>
    <icon>https://eki-ghazouani.github.io/img/favicon.png</icon>
    <entry>
        <title type="html"><![CDATA[Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (2/2)]]></title>
        <id>/2022/10/26/Interpretability_sentiment_analysis_II</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/10/26/Interpretability_sentiment_analysis_II"/>
        <updated>2022-10-26T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two illustrations of how attention coefficients can be a source of interpretability]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a class="hash-link" href="#summary" title="Direct link to heading">​</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. In a first article, we showed how attention coefficients could be the basis of an attribution interpretability method. Here we propose to evaluate how they can also be used to set up counterfactuals. </p></div><div align="justify"><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="work-presented-in-the-previous-article">Work presented in the previous article<a class="hash-link" href="#work-presented-in-the-previous-article" title="Direct link to heading">​</a></h2><p>Previously, the BERT <!-- -->[1]<!-- --> and DistilBERT <!-- -->[2]<!-- --> models have been mobilized to tackle the well-known problem of sentiment analysis. In particular, we have shown that the BERT and DistilBERT models contain within their architecture attention coefficients that can be at the heart of an attribution interpretability method. Starting from an initial text, a visualization of the weight assignment method was proposed. The more red the color, the higher the associated attention coefficient. </p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_2-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg" width="769" height="91" class="img_ev3q"></p><div align="center"> Figure 1 - Attention-Based token importance</div><p>&nbsp;</p><p> We saw that the word groups "<em>favorite movie</em>", "<em>it just never gets old</em>", "<em>performance brings tears</em>", or "<em>it is believable and startling</em>" stood out. This explained well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction. This work was done using the Hugging Face transformers library <!-- -->[3]<!-- -->.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interpreting-through-counterfactual-generation">Interpreting through counterfactual generation<a class="hash-link" href="#interpreting-through-counterfactual-generation" title="Direct link to heading">​</a></h2><p>Another way to do interpretability is to generate counterfactual examples. According to Judea Pearl, counterfactual "involves answering questions which ask what might have been, had circumstances been different” <!-- -->[4]<!-- -->. Thus, the idea is to understand a prediction by generating a counterfactual example, resulting in an opposite prediction. In the context of natural language processing, it is therefore a matter of changing the right words in the review. In order to generate a counterfactual example, we propose the following methodology:</p><ul><li><p>Compute the attention coefficients of the tokens in a text corpus on each attention layer (6). The text corpus size must be statistically significant </p></li><li><p>Perform token clustering based on their 6-dimensional representation</p></li><li><p>Detect clusters associated with positively and negatively charged sentiment words</p></li><li><p>Replace the tokens with the highest average attention with their "opposite token" in their "opposite cluster"
This approach allows us to validate the interpretative strength of the tokens put forward by the attention coefficients, while illustrating what a close review would have been with an opposite sentiment.
We apply the methodology on a corpus of 1000 reviews. The clustering method used is the hierarchical ascending classification (HAC) and gives 3 clusters. The obtained clusters and the counterfactual generation procedure can be represented in 2 dimensions as follows:</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_3-094ab2c3d6b69a58223f9a733aef845b.jpg" width="416" height="342" class="img_ev3q"></p><div align="center"> Figure 2 - Token clusters &amp; replacements</div></li></ul><p>We then generate the counterfactual example of the review tested earlier by changing 2 words: </p><div align="center">delight ➡ torment<p>favorite ➡ worst</p></div><p>This gives us the following counterfactual example:</p><p>“<em>Probably my all time worst movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a torment. the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>".</p><p>As the text is quite long, 2 tokens are not enough to change the feeling associated with the review. The probability score nevertheless drops significantly by 0.3pts.
One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a "counterfactual confusion matrix" as follows:</p><p>One way to assess the quality of the generated counterfactual examples is to evaluate the proportion of reviews in a corpus whose associated sentiment has changed. The result can be represented as a "counterfactual confusion matrix" as follows:</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_4-91c11a737d5e41b98a8657cd3bb17de7.jpg" width="756" height="74" class="img_ev3q"></p><div align="center"> Table 1 - Counterfactual confusion matrix example</div><p>Where :</p><ul><li>X<sub>11</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are positive; sentiment has remained the same </li><li>X<sub>12</sub> represents the share of reviews whose sentiment changed from positive to negative; sentiment did change </li><li>X<sub>21</sub> represents the share of reviews whose sentiment changed from negative to positive; sentiment changed well</li><li>X<sub>22</sub> represents the share of reviews whose initial associated sentiment and the sentiment of the counterfactual example are negative; sentiment has remained the same</li></ul><p>We compute the "counterfactual confusion matrix" on the same text corpus that enabled us to perform clustering, picking 5 tokens for each review. The result is given below:</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_5-de802c493bd17b6c40e5c5714774720e.jpg" width="756" height="73" class="img_ev3q"></p><div align="center"> Table 2 - Actual counterfactual confusion matrix</div><p>&nbsp;</p>Thus, we see that changing the 5 tokens with the highest average attention produces a change in sentiment perception in 44% of cases. In particular, the rate of sentiment change for reviews initially perceived as positive is 31% while the rate of sentiment change for reviews initially perceived as negative is 53%. The change from negative to positive seems to be better achieved with our method.<p>We have shown that attention coefficients can be a source of interpretability. Used in the right way, the attention coefficients allow the detection of tokens with high predictive value. They can also be used to generate counterfactual examples in order to better understand what the sentence should have been in order to be associated with an opposite sentiment. The interest of the attention coefficients is reinforced by the "counterfactual confusion matrix": The high transformation rate of the reviews' sentiments shows that the tokens selected thanks to the attention are strongly meaningful.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-step">Next step<a class="hash-link" href="#next-step" title="Direct link to heading">​</a></h2><p>We plan to test other ways to generate counterfactual examples. One way would be to take advantage of the way DistilBert has been trained: the mask language modeling (MLM). The idea would be to mask the tokens with high average attention, and replace them with the tokens with the highest softmax in the "opposite cluster". This would ensure the grammatical correctness of the generated counterfactual example. Finally, the generation of counterfactual examples can have other applications than interpretability. In particular, it becomes possible to perform data augmentation in order to give more examples to a model. It can mitigate biases by balancing the sentiments of biased discriminated populations. This would improve fairness indicators while not degrading accuracy. </p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><p>[1]<!-- --> VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2]<!-- --> SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3]<!-- --> Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p><p>[4]<!-- --> PEARL, Judea et MACKENZIE, Dana. The book of why: the new science of cause and effect. Basic books, 2018</p></div>]]></content>
        <author>
            <name>Milan Bhan</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="NLP" term="NLP"/>
        <category label="Transformers" term="Transformers"/>
        <category label="BERT" term="BERT"/>
        <category label="interpretability" term="interpretability"/>
        <category label="explainability" term="explainability"/>
        <category label="XAI" term="XAI"/>
        <category label="attention" term="attention"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpreting its sentiment analysis algorithm: BERT and its attention coefficients (1/2)]]></title>
        <id>/2022/10/18/Interpretability_sentiment_analysis_I</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/10/18/Interpretability_sentiment_analysis_I"/>
        <updated>2022-10-18T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Two illustrations of how attention coefficients can be a source of interpretability]]></summary>
        <content type="html"><![CDATA[<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="summary">Summary<a class="hash-link" href="#summary" title="Direct link to heading">​</a></h2><div align="justify"><p>We propose to illustrate how far BERT-type models can be considered as interpretable by design. We show that the attention coefficients specific to BERT architecture constitute a particularly rich piece of information that can be used to perform interpretability. There are mainly two ways to do interpretability: attribution and generation of counterfactual examples. Here we propose to evaluate how attention coefficients can form the basis of an attribution method. We will show in a second article how they can also be used to set up counterfactuals. </p></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-bert-architecture">The BERT architecture<a class="hash-link" href="#the-bert-architecture" title="Direct link to heading">​</a></h2><div align="justify"><p>An artificial neural network is a computer system inspired by the functioning of the human brain and biological neurons to learn specific tasks. The neural networks represent a subset of machine learning algorithms. In order to perform a learning task, the neural network spreads information through an elementary network, called a perceptron. The way in which information is diffused can be formalized through linear algebra and the manipulation of various activation functions. A neural network can be defined as an association of elementary objects called formal neurons, like the perceptron. There are several types of layers that can be part of a neural network:</p><ul><li>Fully connected layers, which receive a vector as input, and produce a new vector as output by applying a linear combination and possibly an activation function;</li><li>Convolution layers, which learn localized patterns in space;</li><li>Attention layers, which model the general relations between different objects.</li></ul></div><div align="justify"><p>Attention mechanisms are particularly effective for natural language processing tasks. This is mainly due to the fact that they allow to properly model a word through mathematical representations. In particular, attention layers make it possible to assign a contextual representation of the word on a case-by-case basis. This makes it a much more efficient tool than Word2vec since the latter only models an average context, but does not adapt to the given situation. Attention mechanisms are at the heart of Transformers-type models as shown in the diagram below. The BERT model corresponds to a stack of the left part of the generic architecture of a Transformer <!-- -->[1]<!-- -->.</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_2-3ae3a962b8d25b8df96ed38b648465b2.jpg" width="221" height="312" class="img_ev3q"></p><div align="center"> Figure 1 - Transformers architecture</div></div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="fine-tuning-of-bert-for-sentiment-analysis">Fine tuning of BERT for sentiment analysis<a class="hash-link" href="#fine-tuning-of-bert-for-sentiment-analysis" title="Direct link to heading">​</a></h2><div align="justify"><p>To illustrate how attention coefficients can be a source of interpretability in natural language processing, we propose to fine tune a DistilBERT for sentiment analysis. A DistilBERT is a distilled version of BERT. It is smaller, faster, cheaper, lighter and recovers 97% of BERT’s performance on GLUE <!-- -->[2]<!-- -->. A perfect compromise, in fact. Most transformers are available pre-trained on the Hugging Face transformers library <!-- -->[3]<!-- -->. The objective is to perform supervised classification on the IMDB database to assess the sentiment associated with a movie review. An illustration of the dataset is shown below:</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_3-5154784fda191b99bf74ac11a362d962.jpg" width="408" height="269" class="img_ev3q"></p><div align="center"> Figure 2 - IMDB sample</div><p>&nbsp;</p>To do so, we import all the libraries needed.  In particular, the tokenizer DistilBertTokenizer and the pre-trained hugging face model TFDistilBertForSequenceClassification are used.<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sentence_encoder = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', output_attentions = True)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>The parameter "output_attention" must be equal to "True". It will allow us to retrieve the attention coefficients of the model. We add a dense layer with a softmax activation to fine tune the model to do sentiment analysis. In order to train the model, we use the following hyperparameters:</p><ul><li>initial_lr  = 1e-5</li><li>n_epochs    = 15</li><li>batch_size  = 64</li><li>random_seed = 42</li></ul><p>Finally, we make evolve the learning and stop the learning process if the val_loss does not decrease after a certain number of iterations. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, verbose = 1,min_delta=0.005,patience=3, min_lr=3e-7)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=6, verbose=1, mode='auto',baseline=None, restore_best_weights=True)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We can finally fine tune the DistilBert.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">history = model.fit(X_train, y_train, batch_size=bs, epochs=n_epochs, validation_data=(X_test, y_test), </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                    verbose=1,callbacks=[early_stop, reduce_lr])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We obtain a val_accurcay of 85%, which is sufficient for our further analysis. Note that a BERT or a RoBERTa would have certainly had a better val_loss, as they are more heavy and complex.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="recovery-of-attention-coefficients">Recovery of attention coefficients<a class="hash-link" href="#recovery-of-attention-coefficients" title="Direct link to heading">​</a></h2><p>We are now able to analyze the attention coefficients related to movie reviews. In order to retrieve it, We need to predict the sentiment associated to a review.
Then, we select the layer(s) of attention to analyze. We focus here on the last layer of attention.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">inputs = tokenizer.batch_encode_plus(reviews,truncation=True, </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                                     add_special_tokens = True, </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                                     max_length = max_len, </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">                                     pad_to_max_length = True)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tokenized = np.array(inputs["input_ids"]).astype("int32")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">attention_mask = np.array(inputs["attention_mask"]).astype("int32")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">encoded_att = model.layers[2](tokenized,attention_mask =attention_mask)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">#last attention layer</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">last_attention=encoded_att.attentions[-1]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>We finally recovered the 12 attention matrices from the last layer of the DistilBert.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="interpreting-through-attention-attribution">Interpreting through attention attribution<a class="hash-link" href="#interpreting-through-attention-attribution" title="Direct link to heading">​</a></h2><p>A first way to take advantage of the attention coefficients is to directly look at their value in order to evaluate if the right words stand out. We choose to calculate the average attention on all attention layers and heads. A more in-depth work of selection of the most relevant layer would allow to refine the interpretability method. Here, we limit ourselves to the most basic case.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">a,b = [], []</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">for head in range(0,12) :</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    for i, elt in enumerate(inputs['input_ids'][0]):</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        if np.array(elt) != 1:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            att = last_attention.numpy()[0,head][0][i]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            a.append(tokenizer.decode([elt]) + '_' + str(i))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            b.append(att)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">attention_all_head=pd.DataFrame({"Token":a,"Attention coefficient":b})</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>In order to have the average attention, we group by the attention score on all the layers and heads.
We finally have the average attention coefficients associated with the words of the film review. As an example, the attention coefficients associated with the following positive review is calculated:</p><p>“<em>Probably my all time favorite movie a story of selflessness sacrifice and dedication to a noble cause but its not preachy or boring . it just never gets old despite my having seen it some 15 or more times in the last 25 years . paul lukas performance brings tears to my eyes and bette davis in one of her very few truly sympathetic roles is a delight . the kids are as grandma says more like dressedup midgets than children but that only makes them more fun to watch . and the mothers slow awakening to whats happening in the world and under her own roof is believable and startling . if i had a dozen thumbs they’d all be up for this movie</em>".</p><p>The review being long, we represent the text in color. The more red the color, the higher the associated attention coefficient. The result is shown below:</p><p> <img loading="lazy" alt="screenshot-app" src="/assets/images/Image_4-fc95973e15821ef99dbc2fd6e4a5b6c8.jpg" width="769" height="91" class="img_ev3q"></p><div align="center"> Figure 3 - Attention-Based token importance</div><p>&nbsp;</p>We see that the word groups "favorite movie", "it just never gets old", "performance brings tears", or "it is believable and startling" stand out. This explains well why the algorithm evaluated the review as positive and what was the semantic field at the root of this prediction.<p>&nbsp;</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="next-step">Next step<a class="hash-link" href="#next-step" title="Direct link to heading">​</a></h2><p>We will show in a future article how attention coefficients are useful for generating counterfactual examples to explain the model prediction.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="references">References<a class="hash-link" href="#references" title="Direct link to heading">​</a></h2><p>[1]<!-- --> VASWANI, Ashish, SHAZEER, Noam, PARMAR, Niki, et al. Attention is all you need. Advances in neural information processing systems, 2017, vol. 30.</p><p>[2]<!-- --> SANH, Victor, DEBUT, Lysandre, CHAUMOND, Julien, et al. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.</p><p>[3]<!-- --> Hugging face library <a href="https://huggingface.co/" target="_blank" rel="noopener noreferrer">https://huggingface.co/</a></p></div>]]></content>
        <author>
            <name>Milan Bhan</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="NLP" term="NLP"/>
        <category label="Transformers" term="Transformers"/>
        <category label="BERT" term="BERT"/>
        <category label="interpretability" term="interpretability"/>
        <category label="explainability" term="explainability"/>
        <category label="XAI" term="XAI"/>
        <category label="attention" term="attention"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Newsletter for September 2022]]></title>
        <id>/2022/09/20/newsletter_Sept-2022</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/09/20/newsletter_Sept-2022"/>
        <updated>2022-09-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, We are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!]]></summary>
        <content type="html"><![CDATA[<div align="justify">Hi everyone, we are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a class="hash-link" href="#data-science" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-warn-customers-when-youre-running-low-on-stock">Why You Should Warn Customers When You’re Running Low on Stock<a class="hash-link" href="#why-you-should-warn-customers-when-youre-running-low-on-stock" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/DS_article_1-e90bd88a7a5908f1218fc44c8eda5ef6.jpg" width="6000" height="4000" class="img_ev3q"></p><div align="justify">The supply-chain disruptions due to the pandemic and the Ukraine war caused the retailers to face unprecedented stockouts risks. To overcome this challenge, Instacart suggest that honesty is the best policy. By using a Machine Learning model to predict that an item is likely out-of-stock and therefore warning clients when it’s the case, they reduced the proportion of replacements and refunds in the short term and increased the total revenue per customer in the long term.</div><p>&nbsp;</p><p><a href="https://hbr.org/2022/09/why-you-should-warn-customers-when-youre-running-low-on-stock" target="_blank" rel="noopener noreferrer">Why You Should Warn Customers When You’re Running Low on Stock - Harvard Business Review</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a class="hash-link" href="#machine-learning" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hopular-modern-hopfield-networks-for-tabular-data">Hopular: Modern Hopfield Networks for Tabular Data<a class="hash-link" href="#hopular-modern-hopfield-networks-for-tabular-data" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/ML_DL-940d62772895a4f7012e4d2b739c908c.jpg" width="5760" height="3840" class="img_ev3q"></p><div align="justify">While Deep Learning excels in structured data as encountered in vision and natural language processing, it failed to meet its expectations on tabular data. For tabular data, Support Vector Machines (SVMs), Random Forests, and Gradient Boosting are the best performing techniques with Gradient Boosting in the lead.</div><div align="justify">"Hopular" is a novel Deep Learning architecture for medium- and small-sized datasets, where each layer is equipped with continuous modern Hopfield networks.</div><div align="justify">In experiments on small-sized tabular datasets with less than 1,000 samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in particular several Deep Learning methods. In experiments on medium-sized tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost, LightGBM and a state-of-the art Deep Learning method designed for tabular data. Thus, Hopular is a strong alternative to these methods on tabular data.</div><p>&nbsp;</p><p><a href="https://ml-jku.github.io/hopular/" target="_blank" rel="noopener noreferrer">GitHub - Hopular</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a class="hash-link" href="#data-engineering--architecture" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-learn-data-engineering">How to learn data engineering<a class="hash-link" href="#how-to-learn-data-engineering" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/DE_article_1-8494ff059bb5e1a03dc20dbb08627e45.jpg" width="4032" height="3024" class="img_ev3q"></p><div align="justify">Like data scientists, data engineers write code. They’re highly analytical, and are interested in data visualization. Unlike data scientists — and inspired by our more mature parent,&nbsp;software engineering&nbsp;— data engineers build tools, infrastructure, frameworks, and services. In fact, it’s arguable that data engineering is much closer to software engineering than it is to a data science.</div><div align="justify">This post introduces the role of a data engineer and the challenges he faces on a daily basis. It also provides some very interesting links to acquire the basics or to consolidate your knowledge in this domain.</div><p>&nbsp;</p><p><a href="https://www.blef.fr/learn-data-engineering/" target="_blank" rel="noopener noreferrer">How to learn data engineering | Blef.fr</a></p><p>&nbsp;</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dos-and-donts-of-data-mesh">Do’s and Don’ts of Data Mesh<a class="hash-link" href="#dos-and-donts-of-data-mesh" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/DE_article_2-746470f3aec65a53368228c3054ef2ee.jpg" width="3072" height="3888" class="img_ev3q"></p><div align="justify">Many enterprises are investing in their next generation data lake, with the hope of democratizing data at scale to provide business insights and ultimately make automated intelligent decisions. Data platforms based on the data lake architecture have common failure modes that lead to unfulfilled promises at scale. To address these failure modes, a new paradigm saw the light : Data Mesh.</div><div align="justify">This article introduces Data Mesh and offers a series of advices, Do’s and Don’ts as a result of its implementation in BlaBlaCar.</div><p>&nbsp;</p><p><a href="https://medium.com/blablacar/dos-and-don-ts-of-data-mesh-e093f1662c2d" target="_blank" rel="noopener noreferrer">Do’s and Don’ts of Data Mesh | Kineret Kimhi | Medium</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a class="hash-link" href="#app-and-web-development" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introducing-signals">Introducing Signals<a class="hash-link" href="#introducing-signals" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/App_article_1-9e30f8f36e826981c668a6b259ee2e00.jpg" width="4077" height="3203" class="img_ev3q"></p><p>Signals are a way of expressing state that ensure apps stay fast regardless of how complex they get. Signals are based on reactive principles and provide excellent developer ergonomics, with a unique implementation optimized for Virtual DOM.</p><p><a href="https://preactjs.com/blog/introducing-signals/" target="_blank" rel="noopener noreferrer">Introducing Signals | Preactjs.com</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="best-practices-for-creating-a-modern-npm-package">Best practices for creating a modern npm package<a class="hash-link" href="#best-practices-for-creating-a-modern-npm-package" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/App_article_2-bcb3b3859a5cc71ff4e0441988e7a445.jpg" width="6336" height="9504" class="img_ev3q"></p><div align="justify">NPM is an online repository for the publishing of open-source Node.js projects, it also allows to interact with the repository through a command-line utility for package installation, version management and dependency management.</div><div align="justify">This article details the best practices for creating and managing an npm package such as security checks, automated semantic version management etc.</div><p>&nbsp;</p><p><a href="hhttps://snyk.io/blog/best-practices-create-modern-npm-package/" target="_blank" rel="noopener noreferrer">Best practices for creating a modern npm package | Brian Clark | Synk.io</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="special-section-responsible-ai">Special Section: Responsible AI<a class="hash-link" href="#special-section-responsible-ai" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openrail-towards-open-and-responsible-ai-licensing-frameworks">OpenRAIL: Towards open and responsible AI licensing frameworks<a class="hash-link" href="#openrail-towards-open-and-responsible-ai-licensing-frameworks" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/responsible-bd81e67654846dc5ebe64a63310811a9.jpg" width="3024" height="4032" class="img_ev3q"></p><div align="justify">Advances in machine learning and other AI-related areas have flourished these past years partly thanks to thethe open source culture. It has in fact allowed knowledge sharing and created communities that fostered innovation. Nevertheless, recent events related to the ethical and socio-economic concerns of development and use of machine learning models have spread a clear message: Making sure AI is responsible is incompatible with open-source. Yet, closed systems are not the answer, as the problem persists under the opacity of firms' private AI development processes.</div><div align="justify">In this context, the OpenRAIL approach suggests a new type of licensing that embed a specific set of restrictions to make sure of the good usage of the models. Therefore, while benefiting from an open access to the ML model, the user will not be able to use the model for the specified restricted scenarios.</div><p>&nbsp;</p><p><a href="https://huggingface.co/blog/open_rail" target="_blank" rel="noopener noreferrer">OpenRAIL: Towards open and responsible AI licensing frameworks | Carlos Munoz Ferrandis | huggingface.co</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="credits">Credits<a class="hash-link" href="#credits" title="Direct link to heading">​</a></h2><ul><li>Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a></li></ul>]]></content>
        <author>
            <name>Ali Ghazouani</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <author>
            <name>Nathan Rouff</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="Data Science" term="Data Science"/>
        <category label="Data Engineering" term="Data Engineering"/>
        <category label="Data Mesh" term="Data Mesh"/>
        <category label="NPM" term="NPM"/>
        <category label="Hopular" term="Hopular"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach]]></title>
        <id>/2022/09/06/deep_rl</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/09/06/deep_rl"/>
        <updated>2022-09-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on an approach based on two famous reinforcement learning algorithms: Q-Learning and Policy Gradient.]]></summary>
        <content type="html"><![CDATA[<p>This article is the second part of the serie of articles introducing optimization techniques for solving the classical Operations Research problem of multi-knapsack. The main objective of this article is to introduce Reinforcement Learning as a way to solve combinatorial optimization problems (Reinforcement Learning can actually be used to solve a much wider range of optimization problems). </p><p>First, the classical Reinforcement Learning framework will be briefly presented. Then, we'll see how to frame the multi-knapsack problem for Reinforcement Learning, followed by explanations on why we chose to explore RL for this combinatorial optimization problem. Eventually, the Q-learning (no neural networks) and Policy Gradient (with neural networks) approaches will be introduced and their performance will be evaluated on the knapsack problem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="reinforcement-learning-for-the-knapsack-problem">Reinforcement Learning for the Knapsack problem<a class="hash-link" href="#reinforcement-learning-for-the-knapsack-problem" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-reinforcement-learning">What is Reinforcement Learning?<a class="hash-link" href="#what-is-reinforcement-learning" title="Direct link to heading">​</a></h3><p>The image below represents the Reinforcement Learning framework. It describes in a simple, yet accurate manner, one of the main ideas behind Reinforcement Learning. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/reinforcement-learning-framework-d89af397a278199d33979dfa5541b2ef.jpg" width="700" height="270" class="img_ev3q"></p><div align="center"> Figure 1 : The Reinforcement Learning framework</div><br><p>Basically, an agent receives information about the state of an environment he evolves in, information we will call S<sub>t</sub> as it describes the state at timestep t.</p><p>Based on this information it receives, the RL agent will choose an action among all the actions it has the right to take at each timestep. We will call such action A<sub>t</sub>, the action at time t, with A<sub>t</sub> belonging to AA<sub>t</sub>(s<sub>t</sub>) the set of available actions given the state S<sub>t</sub>. When an action is taken, it has an impact on the environment and the agent will receive information about the new state of the environment S<sub>t+1</sub> but also a reward to incentivize it to take actions which will maximize the total rewards it expects to obtain at the end of an episode.</p><p>To apply reinforcement learning to solve business problems, these problems have to be framed as a Markov Decision Process, as seen above. More details can be found on how to rigorously define the Reinforcement Learning in the <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver" target="_blank" rel="noopener noreferrer">excellent course given by David Silver</a> (principal research scientist at DeepMind, now owned by Google). You may find his lectures using the previous link, with lectures 1 and 2 being the most pertinent.</p><p>Now, to get a better grasp on how to frame a problem for Reinforcement Learning, let’s consider two practical examples.</p><p>As a first example, we can consider for instance an AI trader. It could have as available actions the possibility to buy or sell many different products. Its actions have an impact on the environment. First, the money it has and the products it owns will be modified, but also if it buys a massive amount of a certain product, it may have an important impact on the future prices. The final goal for it may be to earn as much money as possible. The description of this first example with the prism of Reinforcement Learning is given in figure 2.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Example_AI_Trader_no_logo-c97aec8b49d02c0c4046c4a7da82b008.png" width="1205" height="358" class="img_ev3q"></p><div align="center"> Figure 2 - Example of an AI trader described through the prism of Reinforcement Learning</div><br><p>For the case of a self driving car as the AI agent, the actions it can take could be turning, stopping, accelerating. The information it will receive at each timestep are the speed of the car, its geolocation and probably many others. The environment can be the real world around the car, or just a simulator. The final reward will take into account how fast the car has reached a certain goal position, without damaging things or killing people for instance. Should it damage objects, it could for instance receive negative rewards. This information is summarized in the figure 3 below.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Example_self_driving_car_no_logo-dbd8ade71962beb920fe6122073a96d0.png" width="1205" height="358" class="img_ev3q"></p><div align="center"> Figure 3 - Example of a self driving car described through the prism of Reinforcement Learning</div><br><p>Let’s now tackle the case of the multi-knapsack problem!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-adapt-the-multi-knapsack-problem-for-solving-with-reinforcement-learning">How to adapt the multi-Knapsack problem for solving with Reinforcement Learning<a class="hash-link" href="#how-to-adapt-the-multi-knapsack-problem-for-solving-with-reinforcement-learning" title="Direct link to heading">​</a></h3><p>The precise definition of the multi-knapsack problem was given in the first part of this serie of articles on the knapsack problem. The figure below describes visually the problem at stake.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Knapsack_problem_5-31f9629281a4c78ff57ea1b68c6f753e.png" width="1306" height="1033" class="img_ev3q"></p><div align="center"> Figure 4 - Description of the multi-knapsack problem</div><br><p>In our case, one could think about the agent as a person trying to carefully choose among the many clothes he/she possesses before going on a long trip. The environment would be the empty bags and all the clothes to choose from. At each timestep, the person would have the choice to take one element among the available clothes to put it inside one of the bags, the bags needing to be closed (and thus not to full) before leaving for the trip.</p><p>The objective is to maximize the value of the clothes chosen for the trip.</p><p>And that’s it! Our problem is framed for Reinforcement Learning.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-we-chose-to-explore-rl-for-combinatorial-optimization-problems">Why we chose to explore RL for combinatorial optimization problems<a class="hash-link" href="#why-we-chose-to-explore-rl-for-combinatorial-optimization-problems" title="Direct link to heading">​</a></h3><p>The last two decades have known the breakthrough of Deep Learning which is now massively entering all fields of industry whether this is for Computer Vision, disease predictions, product recommendation, Natural Language Processing applications, etc. Massive investments follow in the field of Machine Learning implying a virtuous circle with more results and regular new breakthroughs. Due to these developments, Deep Reinforcement Learning has emerged from the field of Reinforcement Learning which has been studied for a long time and whose goal is to take actions in an environment in order to maximize a defined cumulative reward. This allowed new recent breakthroughs, such as the AI AlphaGo beating professional Go players in 2016 and more recently AlphaStar beating world champions of the video game Starcraft (more on that in <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" target="_blank" rel="noopener noreferrer">DeepMind blog article</a> or in the <a href="https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D" target="_blank" rel="noopener noreferrer">Nature paper</a>). </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/AlphaStar_Image-a20c844be2e52c1c27cfe7ac9e713edf.png" width="800" height="450" class="img_ev3q"></p><div align="center"> Figure 5 - Visualization highlighting the trained AlphaStar performing against a top Starcraft human players</div><br><p>We’ll say a few words about this video game environment, because this achievement is one of the reasons which motivates the use of Reinforcement Learning for solving combinatorial optimization problems. Indeed, with the knapsack problem, we have a discrete action space with a limited number of actions, although the range of available actions can become extremely high by changing the number of available items and knapsacks. </p><p>Having an AI agent beating the world’s best players on this game is an important breakthrough as this video game environment is extremely complex, with only imperfect information being provided to the agent, the action space being enormous with a choice between up to 10<sup>26</sup> different actions, and actions being taken almost in real time, every 0.2 seconds. Eventually, planning is made on long term and the agent doesn’t know until the end of the game whether it has won the game or not. While applying Deep Reinforcement Learning to video games allows to test the performance of the algorithms very accurately, allowing to judge how it performs in very different environments, applications also begin to appear in other fields, opening the perspective of using these techniques in different industry fields in the next few years. Deep Reinforcement Learning is definitely a field with high potential, and proofs have been shown that it can solve very well high-dimensional problems. Especially, many articles were published where these algorithms were applied to finance problems.</p><p>Now that we have seen the potential of Reinforcement Learning for solving a problem such as the knapsack problem, it is important to keep in mind some important characteristics of Deep Reinforcement Learning approaches: </p><ul><li>Reinforcement Learning algorithms provides us with approximations of the optimal solutions on the contrary to the solutions that could provide Mixed Integer Programming solvers as the ones introduced in the first part of this series of articles on the knapsack problem;</li><li>For the same reason, Reinforcement Learning algorithms will always provide us solutions to the problem, on the contrary to exact methods which could be unable to provide any solution for very complex problems. For that reason, Reinforcement Learning approaches are for instance being developed in order to solve partial differential equations of very high dimensionality, where usual solvers are unable to provide a solution;</li><li>Reinforcement Learning algorithms perform online optimization, meaning that once they have been trained, they are able to solve very complex problems immediately. They have thus tremendous potential for applications which require to solve problems very frequently in a limited time window, such as in trading or product recommendation for instance.</li></ul><p>As seen in the beginning of this section, Reinforcement Learning algorithms have a very high potential for a wide range of business problems. Let's now introduce one of the two Reinforcement Learning approaches used in this notebook. The first one, the Q-learning approach, isn't based on neural networks and doesn't scale well when the dimensionality of the problem increases. We have studied it as it is at the core of other important algorithms such as Deep Q Learning, much more powerful. We will thus concentrate on another promising approach, based on neural networks: the Policy Gradient approach.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="using-q-learning-and-policy-gradient-algorithms-on-the-knapsack-problem">Using Q-Learning and Policy Gradient algorithms on the Knapsack problem<a class="hash-link" href="#using-q-learning-and-policy-gradient-algorithms-on-the-knapsack-problem" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-simple-introduction-to-policy-gradient">A simple introduction to Policy Gradient<a class="hash-link" href="#a-simple-introduction-to-policy-gradient" title="Direct link to heading">​</a></h3><p>The basic principle with a policy gradient approach is that, for each state <em>s</em> received as input, our algorithm will provide us a probability distribution for the actions to take, allowing us to know which object our algorithm recommands us to put first in the knapsack.</p><p>In the formula below, 𝜋 gives us this probability. More precisely, 𝜋 gives us the probability to take action <em>a</em> knowing that we currently at state <em>s</em> and given the 𝜃 values of the model parameters, neurons in our case as we use neural networks. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/policy_gradient_equation_2-ed0850ea2e011caa290c188e973a45ca.svg" width="168" height="45" class="img_ev3q"></p><div align="center"></div><br><p>The use of neural networks isn't mandatory here, but very frequent to obtain good results on complex problems. An example of a representation of a simple neural network is given recalled below.  </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/classical_neural_network_image_2-bc3d584b1817a99f9d27d36fd1ab8d75.png" width="692" height="411" class="img_ev3q"></p><div align="center"> Figure 6 - Representation of a simple neural network</div><br><p>Our model architecture can be visualized with this representation. If we dig a bit deeper into the details of our model's architecture, we have:</p><ul><li><p>As input of the neural network, a description of the current state of the system, that is the value and price information for all items, information about the already selected items and about the current and maximum weight limit inside the different bags;</p></li><li><p>Thanks to this information, our model associates to each possible action a probability and we can then select the action that the algorithm recommends us to take first, that is which object should be stored in which knapsack at the current timestep. This is the output of our model;</p></li><li><p>The parameters of this model, the neurons, are updated at the end of each episode, an episode beginning when all the objects are available and ending when the bags are sufficiently full (or all objects have been selected...). The update of the parameters (𝜃) of the model taking place at the end of each episode only and not each time an action is proposed by the model, the approach is called a <strong>Monte Carlo approach</strong>;</p></li><li><p>The updates of the parameters are made in order to maximize the value of the items stored inside the knapsacks and this approach is based on techniques such as <strong>stochastic gradient descent</strong>.</p></li></ul><p>Now that the Policy Gradient has been described, let's see how our algorithms performed on the knapsack problem!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluating-the-performance-of-the-rl-algorithms">Evaluating the performance of the RL algorithms<a class="hash-link" href="#evaluating-the-performance-of-the-rl-algorithms" title="Direct link to heading">​</a></h3><p>As summarized in figure 7 below, in order to evaluate the performance of the different algorithms, we chose to apply our two RL algorithms (Q-Learning and Policy Gradient) to 3 different environments of increasing difficulty. We trained each algorithm over 400 episodes.</p><p>At the beginning of a new experience, the algorithm had all its coefficients reinitialized. We perform several experiences in order to evaluate how robust is the algorithm. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/evaluation_rl_algos_2-4e81b4faf5f33e55158b446b50a0d583.png" width="3104" height="1218" class="img_ev3q"></p><div align="center"> Figure 7 - Description of the evaluation process for the different algorithms</div><br><p>Eventually, we evaluated the performance of the algorithms with 3 metrics:</p><ul><li>The mean reward shows how good on average the algorithm is;</li><li>The standard deviation highlights the potential lack of robustness of the algorithm;</li><li>The performance ratio RL vs MILP tells us how close is the RL algorithm to the optimal solution provided by a MILP solver (details on how to obtain such a solution are given in the notebook). </li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results-with-q-learning-no-neural-networks">Results with Q-Learning (no neural networks)<a class="hash-link" href="#results-with-q-learning-no-neural-networks" title="Direct link to heading">​</a></h3><p>The graphs on the left show that overall our Q-Learning algorithm does indeed improve through training as its reward improved over time. However, we can see that the performance ratio RL vs MILP is very low, meaning that it is far from achieving as good results as what we could get using state-of-the-art MILP solvers.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/performance_q_learning_knapsack_4-9dde740d1774022a4a3d0668e3ae8e78.png" width="3262" height="1697" class="img_ev3q"></p><div align="center"> Figure 8 - Performance of the Q-Learning algorithm</div><br><p>Furthermore, environments of increasing complexity / dimensionality will be much more difficult to handle for Q-Learning, as its Q-value matrix has as number of columns the number of items multiplied by the number of knapsacks and as rows all the possible states which could exist. Increasing only slightly the number of knapsacks or bags will thus quickly make the Q-Learning algorithm unusable.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results-with-policy-gradient-based-on-neural-networks">Results with Policy Gradient (based on neural networks)<a class="hash-link" href="#results-with-policy-gradient-based-on-neural-networks" title="Direct link to heading">​</a></h3><p>The results of the learning process with Policy Gradient are much better than with Q-learning. At the beginning, due to random initialization of the neural network parameters, the actions are taken at random and the reward is very low, but it quickly improves until reaching a local maximum, not global as it is still lower than the solution obtained with the MILP solver. </p><p>The performance ratio is quite good on the three different environments, reaching approximately 80% for each. The algorithm scales well when the complexity increases.</p><p>We see however that the standard deviation is quite high, which highlights the fact that each time a model is initialized, it can converge to quite different values. It is thus not extremely robust and several initializations are required before finding good results approaching the optimal solution.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/performance_policy_gradient_knapsack_2-dc5ac694619364f3bad0e9927fe54ad1.png" width="2944" height="1674" class="img_ev3q"></p><div align="center"> Figure 9 - Performance of the REINFORCE (policy-gradient approach) algorithm</div><br><p>On the graph below are highlighted some of the limitations we have witnessed with Policy Gradient algorithms such as the REINFORCE algorithm. We have a lack of robustness, having our algorithm sometimes working very well, sometimes leading to a poorer reward.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/robustness_pb_policy_gradient_2-ccd31fb5440a6da8290fa43122747787.png" width="713" height="394" class="img_ev3q"></p><div align="center"> Figure 10 - REINFORCE algorithm (gradient-policy approach) appears as lacking robustness</div><br>For that reason, the hyperparameter tuning is made more complicated. Indeed comparing one combination of hyperparameters with another one isn't enough to be certain about which combination of hyperparameters is the best, because of the high variability of results for fixed hyperparameters.<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="opening">Opening<a class="hash-link" href="#opening" title="Direct link to heading">​</a></h2><p>For obtaining the results given in this article, we reproduced classical Reinforcement Learning algorithms: the Q-learning algorithm which does not rely on the use of neural networks, and a Policy Gradient algorithm which relies on neural networks. We saw that the latter obtained much better results than the former as we could expect. </p><p>We also built our own multi-knapsack environment file, allowing us to easily modify greatly the complexity of the environment by increasing both the number of available items to put in the knapsacks and the number of knapsacks. </p><p>While creating the environment file, we followed the nomenclature proposed by Open AI Gym for building Reinforcement Learning environments, using the same method names used to define an Open AI Gym environment. The objective was to be able to experiment much more quickly in the future by making use of one of the different Deep Reinforcement Learning libraries (Stable Baselines, TF Agents, Tensorforce…). Indeed, these libraries allow access to many different advanced Deep Reinforcement Learning algorithms already implemented, which can directly be used on new problems if the environment file describing the problem has been built using Open AI Gym nomenclature.</p><p>Another article will be written soon to tell more about how to perform hyperparameter tuning for RL using the hyperparameter optimization framework <a href="https://optuna.org/" target="_blank" rel="noopener noreferrer">Optuna</a> and how to compare and evaluate the efficiency of many different RL algorithms using <a href="https://stable-baselines3.readthedocs.io/en/master/" target="_blank" rel="noopener noreferrer">Stable Baselines</a>!</p>]]></content>
        <author>
            <name>Nathan Rouff</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="Operational Research" term="Operational Research"/>
        <category label="Optimization" term="Optimization"/>
        <category label="Knapsack problem" term="Knapsack problem"/>
        <category label="Deep Reinforcement Learning" term="Deep Reinforcement Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep RL and Optimization applied to Operations Research problem - 1/2 Traditional Optimization techniques]]></title>
        <id>/2022/08/27/traditional_or</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/08/27/traditional_or"/>
        <updated>2022-08-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on traditional optimization techniques.]]></summary>
        <content type="html"><![CDATA[<p>Let <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f\colon[a,b]\to\R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace nobreak"></span><span class="mspace" style="margin-right:0.1111em"></span><span class="mpunct"></span><span class="mspace" style="margin-right:-0.1667em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mrel">:</span></span><span class="mspace" style="margin-right:0.3333em"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord mathbb">R</span></span></span></span></span> be Riemann integrable. Let <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">F\colon[a,b]\to\R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mspace nobreak"></span><span class="mspace" style="margin-right:0.1111em"></span><span class="mpunct"></span><span class="mspace" style="margin-right:-0.1667em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mrel">:</span></span><span class="mspace" style="margin-right:0.3333em"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord mathbb">R</span></span></span></span></span> be
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∫</mo><mi>a</mi><mi>x</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">F(x)=\int_{a}^{x} f(t)\,dt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2151em;vertical-align:-0.3558em"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0006em">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8593em"><span style="top:-2.3442em;margin-left:-0.1945em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span style="top:-3.2579em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3558em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">t</span></span></span></span></span>. Then <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> is continuous, and at all <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> such that
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span></span> is continuous at <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> is differentiable at <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> with <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F'(x)=f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mrow><mn>2</mn><mi>π</mi></mrow></msubsup><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">I = \int_0^{2\pi} \sin(x)\,dx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.476em;vertical-align:-0.9119em"></span><span class="mop"><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011em">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.564em"><span style="top:-1.7881em;margin-left:-0.4445em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.8129em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9119em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span></span></span></span></span></div><p>In this first article is introduced a systematic way to approach and solve optimization problems. Then, the multi-knapsack problem itself is introduced. Then we apply the rules defined before on how to solve optimization problems and obtain the optimal solution to the multi-knapsack problem, formulated as a Mixed Integer problem and using Python-MIP package. Let's now introduce simple steps one can follow to approach optimization problems with optimization solvers.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="main-steps-while-creating-an-optimization-model-to-solve-a-business-problem">Main steps while creating an optimization model to solve a business problem<a class="hash-link" href="#main-steps-while-creating-an-optimization-model-to-solve-a-business-problem" title="Direct link to heading">​</a></h2><p>Once a business problem that could benefit from optimization has been identified, we can define a systematic approach based on 3 steps for solving all kind of optimization problems with optimization solvers. These 3 steps are highlighted in the figure below.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/3_steps_math_modelling_4-679cfdabcdf2ab022e54b439d00a8992.png" width="2670" height="567" class="img_ev3q"></p><div align="center"> Figure 1 : The 3 main steps for solving a business problem through optimization</div><br><p>In more details, these 3 steps are: </p><ol><li><p><strong>Create the conceptual mathematical model</strong> that defines the different variables, constraints, etc. in the business problem. This step consists in writing down on paper the equations that define our problem. </p></li><li><p><strong>Translate the conceptual mathematical model into a computer program</strong>. For most programming languages used for optimization, the computer program will largely resembles the mathematical equations one would write on paper.</p></li><li><p><strong>Solve the mathematical model using a math programming solver</strong>. The solver available for Mathematical Programming (solvers such as GLPK, Gurobi, CPLEX...) relies on very sophisticated algorithms. Important algorithms and ideas used in these solvers are, among many others: simplex method, branch &amp; bound, use of heuristics...</p></li></ol><p>Let's see those 3 steps for the case of the multi-knapsack problem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-multi-knapsack-problem">The multi-knapsack problem<a class="hash-link" href="#the-multi-knapsack-problem" title="Direct link to heading">​</a></h2><p>The objective here is, given a set of <em>n</em> items and a set of <em>m</em> knapsacks, to <strong>maximize</strong> the total value of the items put in the knapsacks without exceeding their capacity.</p><p>Below,  w<sub>i</sub> represents the weight of item i,  p<sub>i</sub> the value of item i while  c<sub>j</sub> represents the capacity of knapsack j.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Knapsack_problem_5-31f9629281a4c78ff57ea1b68c6f753e.png" width="1306" height="1033" class="img_ev3q"></p><div align="center"> Figure 2: Description of the multi-knapsack problem</div><br><p>The multi-knapsack is an extension of the classical knapsack problem where instead of considering only one knapsack, we consider as many as we want. This allows to easily extend the complexity of this problem.</p><p>While the problem is relatively easy to define mathematically, it belongs to the class of NP-hard problems. Without going into the details of what defines NP-hard problems, we can easily see that the complexity of the knapsack problems explodes when the number of knapsacks and items increases. Indeed, we have m<sup>n</sup> available combinations we would need to test should we want to apply a brute-force approach for solving this problem. Just with 10 knapsacks and 80 items, there are 10<sup>80</sup> combinations, which is the estimation of the number of atoms in the universe! And 10 knapsacks and 80 items is still quite limited... Let's now try to create the conceptual mathematical model by defining the problem with equations.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="creating-the-conceptual-mathematical-model">Creating the conceptual mathematical model<a class="hash-link" href="#creating-the-conceptual-mathematical-model" title="Direct link to heading">​</a></h3><p>A quick translation of the multi-knapsack problem with equation can be written as the following: </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/equations_1-de05b24a1b63925c7e9861db33be2341.svg" width="262" height="161" class="img_ev3q">
<img loading="lazy" alt="screenshot-app" src="/assets/images/equations_3-8e121b52240ee3c6995bc91941f72164.svg" width="201" height="19" class="img_ev3q"></p><p>Now that we managed to translate the problem into a set of equations, let's translate this mathematical model so that it is understood by a computer program. Below, we will make use of the Python package <a href="https://www.python-mip.com/" target="_blank" rel="noopener noreferrer">Python-MIP</a> which is open-source and provides tools for modeling and solving Mixed-Integer Linear Programming Problems (MIP), relying on fast open source solvers.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="translating-the-mathematical-model-into-a-computer-program-with-python-mip">Translating the mathematical model into a computer program with Python-MIP<a class="hash-link" href="#translating-the-mathematical-model-into-a-computer-program-with-python-mip" title="Direct link to heading">​</a></h3><p>Before solving the problem, we have to generate an instance for it (have data defining the problem). To do so, you can use the following code that will generate an instance of this problem with 40 items to store in 5 bags.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">import pandas as pd</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import numpy as np</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import pickle</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">def data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag):</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data = {}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    weights = np.random.randint(minimum_weight_item, maximum_weight_item, size = number_items)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    values = np.random.randint(minimum_value_item, maximum_value_item, size = number_items)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['weights'] = weights</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['values'] = values</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['items'] = list(range(len(weights)))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['num_items'] = len(weights)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['bins'] = list(range(number_bags))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['bin_capacities'] = np.random.randint(0, max_weight_bag, size = number_bags) + np.int(np.mean(data['weights']))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    return(data)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">number_bags = 5</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">number_items = 40</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">minimum_weight_item = 0</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">maximum_weight_item = 75</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">minimum_value_item = 0</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">maximum_value_item = 75</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">max_weight_bag = 150</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">data = data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Let's now import the package used to have access to the MIP solver, here using the python package Python-MIP:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">from mip import Model, xsum, maximize, BINARY</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, we can translate the mathematical model so that it is understood by Python-MIP. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">def mip_solve_knapsack(data):</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  model = Model("knapsack")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  x = [[model.add_var(var_type=BINARY) for i in data['items']] for j in data['bins']]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  model.objective = maximize(xsum((xsum(data['values'][i] * x[j][i] for i in data['items']) for j in data['bins'])))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  for j in data['bins']:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      model += xsum(data['weights'][i] * x[j][i] for i in data['items']) &lt;= data['bin_capacities'][j]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  # Each item can be in at most one bin</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  for i in data['items']:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      model += xsum(x[j][i] for j in data['bins']) &lt;= 1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  model.optimize()</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  return(model)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Remark how close it is from the original equations! These solvers are very powerful and yet easy to use directly in Python. The code is indeed very close to the original equations. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-mathematical-model-with-python-mip">Solving the mathematical model with Python-MIP<a class="hash-link" href="#solving-the-mathematical-model-with-python-mip" title="Direct link to heading">​</a></h3><p>Using the <strong>mip_solve_knapsack</strong> function defined in the previous section, we can access to important information regarding the problem, such as the final objective value and the values of x<sub>ij</sub> telling us what were the best combinations of items inside knapsacks.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="some-mathematical-optimization-packages">Some Mathematical Optimization packages<a class="hash-link" href="#some-mathematical-optimization-packages" title="Direct link to heading">​</a></h3><p>In the notebook associated to this article, the package Python-MIP was used. <strong>Python-MIP</strong> is free, but many other packages exist for solving optimization problems on Python (and other languages of course like Julia). For instance <strong>OR-Tools</strong> from Google is a well-recognized free solver, with <a href="https://developers.google.com/optimization/introduction/overview" target="_blank" rel="noopener noreferrer">detailed documentation</a>. </p><p>On the other side, <strong>Gurobi</strong> is a very popular commercial solution for mathematical optimization and its documentation is extremely rich, with quick introductions about <a href="https://www.gurobi.com/resource/modeling-basics/" target="_blank" rel="noopener noreferrer">Mathematical Programming</a>, <a href="https://www.gurobi.com/resource/mip-basics/" target="_blank" rel="noopener noreferrer">Linear Programming</a> and <a href="https://www.gurobi.com/resource/mip-basics/" target="_blank" rel="noopener noreferrer">Mixed-Integer Programming</a>. Importantly, it has a <a href="https://www.gurobi.com/resource/modeling-examples-using-the-gurobi-python-api-in-jupyter-notebook/" target="_blank" rel="noopener noreferrer">large number of modeling examples from all industry fields</a> directly available on Google Colab allowing to better grasp notions of Mathematical Modelling and to improve modeling skills to tackle all kind of optimization problems with Python. This resource can be of use even if one doesn't plan to use this commercial software but rather a free package such as OR-Tools.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>In this article was introduced the multi-knapsack problem, an NP-complete problem, very difficult to solve when taking many items and bags. </p><p>The approach to solve the multi-knapsack problem relied on Python-MIP, a free optimization package using powerful MILP solvers to solve very efficiently all kinds of optimization problems.</p><p>In the next part of this series on the multi-knapsack problem, well studied in the field of Operations Research and at the heart of many real optimization problems, we'll highlight how Deep Reinforcement Learning can be used in order to solve combinatorial optimization problems such as this one. Stay tuned!</p>]]></content>
        <author>
            <name>Nathan Rouff</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="Operational Research" term="Operational Research"/>
        <category label="Optimization" term="Optimization"/>
        <category label="Knapsack problem" term="Knapsack problem"/>
        <category label="Solvers" term="Solvers"/>
    </entry>
</feed>