<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://eki-ghazouani.github.io/blog</id>
    <title>Eki.Lab Blog</title>
    <updated>2022-09-20T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://eki-ghazouani.github.io/blog"/>
    <subtitle>Eki.Lab Blog</subtitle>
    <icon>https://eki-ghazouani.github.io/img/favicon.png</icon>
    <entry>
        <title type="html"><![CDATA[Newsletter for September 2022]]></title>
        <id>/2022/09/20/newsletter_Sept-2022</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/09/20/newsletter_Sept-2022"/>
        <updated>2022-09-20T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Hi everyone, We are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!]]></summary>
        <content type="html"><![CDATA[<div><meta property="og:image" content="./img/blog/header.png"></div><div align="justify">Hi everyone, we are now in September and we release our 6th Newsletter! Ranging from podcasts to tutorials, this Newsletter is made for practicioners!</div><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-science">Data Science<a class="hash-link" href="#data-science" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-you-should-warn-customers-when-youre-running-low-on-stock">Why You Should Warn Customers When You’re Running Low on Stock<a class="hash-link" href="#why-you-should-warn-customers-when-youre-running-low-on-stock" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/DS_article_1-e90bd88a7a5908f1218fc44c8eda5ef6.jpg" width="6000" height="4000" class="img_ev3q"></p><div align="justify">The supply-chain disruptions due to the pandemic and the Ukraine war caused the retailers to face unprecedented stockouts risks. To overcome this challenge, Instacart suggest that honesty is the best policy. By using a Machine Learning model to predict that an item is likely out-of-stock and therefore warning clients when it’s the case, they reduced the proportion of replacements and refunds in the short term and increased the total revenue per customer in the long term.</div><p>&nbsp;</p><p><a href="https://hbr.org/2022/09/why-you-should-warn-customers-when-youre-running-low-on-stock" target="_blank" rel="noopener noreferrer">Why You Should Warn Customers When You’re Running Low on Stock - Harvard Business Review</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="machine-learning">Machine Learning<a class="hash-link" href="#machine-learning" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="hopular-modern-hopfield-networks-for-tabular-data">Hopular: Modern Hopfield Networks for Tabular Data<a class="hash-link" href="#hopular-modern-hopfield-networks-for-tabular-data" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/ML_DL-940d62772895a4f7012e4d2b739c908c.jpg" width="5760" height="3840" class="img_ev3q"></p><div align="justify">While Deep Learning excels in structured data as encountered in vision and natural language processing, it failed to meet its expectations on tabular data. For tabular data, Support Vector Machines (SVMs), Random Forests, and Gradient Boosting are the best performing techniques with Gradient Boosting in the lead.</div><div align="justify">"Hopular" is a novel Deep Learning architecture for medium- and small-sized datasets, where each layer is equipped with continuous modern Hopfield networks.</div><div align="justify">In experiments on small-sized tabular datasets with less than 1,000 samples, Hopular surpasses Gradient Boosting, Random Forests, SVMs, and in particular several Deep Learning methods. In experiments on medium-sized tabular data with about 10,000 samples, Hopular outperforms XGBoost, CatBoost, LightGBM and a state-of-the art Deep Learning method designed for tabular data. Thus, Hopular is a strong alternative to these methods on tabular data.</div><p>&nbsp;</p><p><a href="https://ml-jku.github.io/hopular/" target="_blank" rel="noopener noreferrer">GitHub - Hopular</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="data-engineering--architecture">Data Engineering &amp; Architecture<a class="hash-link" href="#data-engineering--architecture" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-learn-data-engineering">How to learn data engineering<a class="hash-link" href="#how-to-learn-data-engineering" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/DE_article_1-8494ff059bb5e1a03dc20dbb08627e45.jpg" width="4032" height="3024" class="img_ev3q"></p><div align="justify">Like data scientists, data engineers write code. They’re highly analytical, and are interested in data visualization. Unlike data scientists — and inspired by our more mature parent,&nbsp;software engineering&nbsp;— data engineers build tools, infrastructure, frameworks, and services. In fact, it’s arguable that data engineering is much closer to software engineering than it is to a data science.</div><div align="justify">This post introduces the role of a data engineer and the challenges he faces on a daily basis. It also provides some very interesting links to acquire the basics or to consolidate your knowledge in this domain.</div><p>&nbsp;</p><p><a href="https://www.blef.fr/learn-data-engineering/" target="_blank" rel="noopener noreferrer">How to learn data engineering | Blef.fr</a></p><p>&nbsp;</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="dos-and-donts-of-data-mesh">Do’s and Don’ts of Data Mesh<a class="hash-link" href="#dos-and-donts-of-data-mesh" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/DE_article_2-746470f3aec65a53368228c3054ef2ee.jpg" width="3072" height="3888" class="img_ev3q"></p><div align="justify">Many enterprises are investing in their next generation data lake, with the hope of democratizing data at scale to provide business insights and ultimately make automated intelligent decisions. Data platforms based on the data lake architecture have common failure modes that lead to unfulfilled promises at scale. To address these failure modes, a new paradigm saw the light : Data Mesh.</div><div align="justify">This article introduces Data Mesh and offers a series of advices, Do’s and Don’ts as a result of its implementation in BlaBlaCar.</div><p>&nbsp;</p><p><a href="https://medium.com/blablacar/dos-and-don-ts-of-data-mesh-e093f1662c2d" target="_blank" rel="noopener noreferrer">Do’s and Don’ts of Data Mesh | Kineret Kimhi | Medium</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="app-and-web-development">App and Web Development<a class="hash-link" href="#app-and-web-development" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="introducing-signals">Introducing Signals<a class="hash-link" href="#introducing-signals" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/App_article_1-9e30f8f36e826981c668a6b259ee2e00.jpg" width="4077" height="3203" class="img_ev3q"></p><p>Signals are a way of expressing state that ensure apps stay fast regardless of how complex they get. Signals are based on reactive principles and provide excellent developer ergonomics, with a unique implementation optimized for Virtual DOM.</p><p><a href="https://preactjs.com/blog/introducing-signals/" target="_blank" rel="noopener noreferrer">Introducing Signals | Preactjs.com</a></p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="best-practices-for-creating-a-modern-npm-package">Best practices for creating a modern npm package<a class="hash-link" href="#best-practices-for-creating-a-modern-npm-package" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/App_article_2-bcb3b3859a5cc71ff4e0441988e7a445.jpg" width="6336" height="9504" class="img_ev3q"></p><div align="justify">NPM is an online repository for the publishing of open-source Node.js projects, it also allows to interact with the repository through a command-line utility for package installation, version management and dependency management.</div><div align="justify">This article details the best practices for creating and managing an npm package such as security checks, automated semantic version management etc.</div><p>&nbsp;</p><p><a href="hhttps://snyk.io/blog/best-practices-create-modern-npm-package/" target="_blank" rel="noopener noreferrer">Best practices for creating a modern npm package | Brian Clark | Synk.io</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="special-section-responsible-ai">Special Section: Responsible AI<a class="hash-link" href="#special-section-responsible-ai" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="openrail-towards-open-and-responsible-ai-licensing-frameworks">OpenRAIL: Towards open and responsible AI licensing frameworks<a class="hash-link" href="#openrail-towards-open-and-responsible-ai-licensing-frameworks" title="Direct link to heading">​</a></h3><p><img loading="lazy" src="/assets/images/responsible-bd81e67654846dc5ebe64a63310811a9.jpg" width="3024" height="4032" class="img_ev3q"></p><div align="justify">Advances in machine learning and other AI-related areas have flourished these past years partly thanks to thethe open source culture. It has in fact allowed knowledge sharing and created communities that fostered innovation. Nevertheless, recent events related to the ethical and socio-economic concerns of development and use of machine learning models have spread a clear message: Making sure AI is responsible is incompatible with open-source. Yet, closed systems are not the answer, as the problem persists under the opacity of firms' private AI development processes.</div><div align="justify">In this context, the OpenRAIL approach suggests a new type of licensing that embed a specific set of restrictions to make sure of the good usage of the models. Therefore, while benefiting from an open access to the ML model, the user will not be able to use the model for the specified restricted scenarios.</div><p>&nbsp;</p><p><a href="https://huggingface.co/blog/open_rail" target="_blank" rel="noopener noreferrer">OpenRAIL: Towards open and responsible AI licensing frameworks | Carlos Munoz Ferrandis | huggingface.co</a></p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="credits">Credits<a class="hash-link" href="#credits" title="Direct link to heading">​</a></h2><ul><li>Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a></li></ul>]]></content>
        <author>
            <name>Ali Ghazouani</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="Data Science" term="Data Science"/>
        <category label="Data Engineering" term="Data Engineering"/>
        <category label="Data Mesh" term="Data Mesh"/>
        <category label="NPM" term="NPM"/>
        <category label="Hopular" term="Hopular"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep RL and Optimization applied to Operations Research problem - 2/2 Reinforcement Learning approach]]></title>
        <id>/2022/09/06/deep_rl</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/09/06/deep_rl"/>
        <updated>2022-09-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on an approach based on two famous reinforcement learning algorithms: Q-Learning and Policy Gradient.]]></summary>
        <content type="html"><![CDATA[<p>This article is the second part of the serie of articles introducing optimization techniques for solving the classical Operations Research problem of multi-knapsack. The main objective of this article is to introduce Reinforcement Learning as a way to solve combinatorial optimization problems (Reinforcement Learning can actually be used to solve a much wider range of optimization problems). </p><p>First, the classical Reinforcement Learning framework will be briefly presented. Then, we'll see how to frame the multi-knapsack problem for Reinforcement Learning, followed by explanations on why we chose to explore RL for this combinatorial optimization problem. Eventually, the Q-learning (no neural networks) and Policy Gradient (with neural networks) approaches will be introduced and their performance will be evaluated on the knapsack problem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="reinforcement-learning-for-the-knapsack-problem">Reinforcement Learning for the Knapsack problem<a class="hash-link" href="#reinforcement-learning-for-the-knapsack-problem" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="what-is-reinforcement-learning">What is Reinforcement Learning?<a class="hash-link" href="#what-is-reinforcement-learning" title="Direct link to heading">​</a></h3><p>The image below represents the Reinforcement Learning framework. It describes in a simple, yet accurate manner, one of the main ideas behind Reinforcement Learning. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/reinforcement-learning-framework-d89af397a278199d33979dfa5541b2ef.jpg" width="700" height="270" class="img_ev3q"></p><div align="center"> Figure 1 : The Reinforcement Learning framework</div><br><p>Basically, an agent receives information about the state of an environment he evolves in, information we will call S<sub>t</sub> as it describes the state at timestep t.</p><p>Based on this information it receives, the RL agent will choose an action among all the actions it has the right to take at each timestep. We will call such action A<sub>t</sub>, the action at time t, with A<sub>t</sub> belonging to AA<sub>t</sub>(s<sub>t</sub>) the set of available actions given the state S<sub>t</sub>. When an action is taken, it has an impact on the environment and the agent will receive information about the new state of the environment S<sub>t+1</sub> but also a reward to incentivize it to take actions which will maximize the total rewards it expects to obtain at the end of an episode.</p><p>To apply reinforcement learning to solve business problems, these problems have to be framed as a Markov Decision Process, as seen above. More details can be found on how to rigorously define the Reinforcement Learning in the <a href="https://deepmind.com/learning-resources/-introduction-reinforcement-learning-david-silver" target="_blank" rel="noopener noreferrer">excellent course given by David Silver</a> (principal research scientist at DeepMind, now owned by Google). You may find his lectures using the previous link, with lectures 1 and 2 being the most pertinent.</p><p>Now, to get a better grasp on how to frame a problem for Reinforcement Learning, let’s consider two practical examples.</p><p>As a first example, we can consider for instance an AI trader. It could have as available actions the possibility to buy or sell many different products. Its actions have an impact on the environment. First, the money it has and the products it owns will be modified, but also if it buys a massive amount of a certain product, it may have an important impact on the future prices. The final goal for it may be to earn as much money as possible. The description of this first example with the prism of Reinforcement Learning is given in figure 2.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Example_AI_Trader_no_logo-c97aec8b49d02c0c4046c4a7da82b008.png" width="1205" height="358" class="img_ev3q"></p><div align="center"> Figure 2 - Example of an AI trader described through the prism of Reinforcement Learning</div><br><p>For the case of a self driving car as the AI agent, the actions it can take could be turning, stopping, accelerating. The information it will receive at each timestep are the speed of the car, its geolocation and probably many others. The environment can be the real world around the car, or just a simulator. The final reward will take into account how fast the car has reached a certain goal position, without damaging things or killing people for instance. Should it damage objects, it could for instance receive negative rewards. This information is summarized in the figure 3 below.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Example_self_driving_car_no_logo-dbd8ade71962beb920fe6122073a96d0.png" width="1205" height="358" class="img_ev3q"></p><div align="center"> Figure 3 - Example of a self driving car described through the prism of Reinforcement Learning</div><br><p>Let’s now tackle the case of the multi-knapsack problem!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="how-to-adapt-the-multi-knapsack-problem-for-solving-with-reinforcement-learning">How to adapt the multi-Knapsack problem for solving with Reinforcement Learning<a class="hash-link" href="#how-to-adapt-the-multi-knapsack-problem-for-solving-with-reinforcement-learning" title="Direct link to heading">​</a></h3><p>The precise definition of the multi-knapsack problem was given in the first part of this serie of articles on the knapsack problem. The figure below describes visually the problem at stake.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Knapsack_problem_5-31f9629281a4c78ff57ea1b68c6f753e.png" width="1306" height="1033" class="img_ev3q"></p><div align="center"> Figure 4 - Description of the multi-knapsack problem</div><br><p>In our case, one could think about the agent as a person trying to carefully choose among the many clothes he/she possesses before going on a long trip. The environment would be the empty bags and all the clothes to choose from. At each timestep, the person would have the choice to take one element among the available clothes to put it inside one of the bags, the bags needing to be closed (and thus not to full) before leaving for the trip.</p><p>The objective is to maximize the value of the clothes chosen for the trip.</p><p>And that’s it! Our problem is framed for Reinforcement Learning.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="why-we-chose-to-explore-rl-for-combinatorial-optimization-problems">Why we chose to explore RL for combinatorial optimization problems<a class="hash-link" href="#why-we-chose-to-explore-rl-for-combinatorial-optimization-problems" title="Direct link to heading">​</a></h3><p>The last two decades have known the breakthrough of Deep Learning which is now massively entering all fields of industry whether this is for Computer Vision, disease predictions, product recommendation, Natural Language Processing applications, etc. Massive investments follow in the field of Machine Learning implying a virtuous circle with more results and regular new breakthroughs. Due to these developments, Deep Reinforcement Learning has emerged from the field of Reinforcement Learning which has been studied for a long time and whose goal is to take actions in an environment in order to maximize a defined cumulative reward. This allowed new recent breakthroughs, such as the AI AlphaGo beating professional Go players in 2016 and more recently AlphaStar beating world champions of the video game Starcraft (more on that in <a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii" target="_blank" rel="noopener noreferrer">DeepMind blog article</a> or in the <a href="https://www.nature.com/articles/s41586-019-1724-z.epdf?author_access_token=lZH3nqPYtWJXfDA10W0CNNRgN0jAjWel9jnR3ZoTv0PSZcPzJFGNAZhOlk4deBCKzKm70KfinloafEF1bCCXL6IIHHgKaDkaTkBcTEv7aT-wqDoG1VeO9-wO3GEoAMF9bAOt7mJ0RWQnRVMbyfgH9A%3D%3D" target="_blank" rel="noopener noreferrer">Nature paper</a>). </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/AlphaStar_Image-a20c844be2e52c1c27cfe7ac9e713edf.png" width="800" height="450" class="img_ev3q"></p><div align="center"> Figure 5 - Visualization highlighting the trained AlphaStar performing against a top Starcraft human players</div><br><p>We’ll say a few words about this video game environment, because this achievement is one of the reasons which motivates the use of Reinforcement Learning for solving combinatorial optimization problems. Indeed, with the knapsack problem, we have a discrete action space with a limited number of actions, although the range of available actions can become extremely high by changing the number of available items and knapsacks. </p><p>Having an AI agent beating the world’s best players on this game is an important breakthrough as this video game environment is extremely complex, with only imperfect information being provided to the agent, the action space being enormous with a choice between up to 10<sup>26</sup> different actions, and actions being taken almost in real time, every 0.2 seconds. Eventually, planning is made on long term and the agent doesn’t know until the end of the game whether it has won the game or not. While applying Deep Reinforcement Learning to video games allows to test the performance of the algorithms very accurately, allowing to judge how it performs in very different environments, applications also begin to appear in other fields, opening the perspective of using these techniques in different industry fields in the next few years. Deep Reinforcement Learning is definitely a field with high potential, and proofs have been shown that it can solve very well high-dimensional problems. Especially, many articles were published where these algorithms were applied to finance problems.</p><p>Now that we have seen the potential of Reinforcement Learning for solving a problem such as the knapsack problem, it is important to keep in mind some important characteristics of Deep Reinforcement Learning approaches: </p><ul><li>Reinforcement Learning algorithms provides us with approximations of the optimal solutions on the contrary to the solutions that could provide Mixed Integer Programming solvers as the ones introduced in the first part of this series of articles on the knapsack problem;</li><li>For the same reason, Reinforcement Learning algorithms will always provide us solutions to the problem, on the contrary to exact methods which could be unable to provide any solution for very complex problems. For that reason, Reinforcement Learning approaches are for instance being developed in order to solve partial differential equations of very high dimensionality, where usual solvers are unable to provide a solution;</li><li>Reinforcement Learning algorithms perform online optimization, meaning that once they have been trained, they are able to solve very complex problems immediately. They have thus tremendous potential for applications which require to solve problems very frequently in a limited time window, such as in trading or product recommendation for instance.</li></ul><p>As seen in the beginning of this section, Reinforcement Learning algorithms have a very high potential for a wide range of business problems. Let's now introduce one of the two Reinforcement Learning approaches used in this notebook. The first one, the Q-learning approach, isn't based on neural networks and doesn't scale well when the dimensionality of the problem increases. We have studied it as it is at the core of other important algorithms such as Deep Q Learning, much more powerful. We will thus concentrate on another promising approach, based on neural networks: the Policy Gradient approach.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="using-q-learning-and-policy-gradient-algorithms-on-the-knapsack-problem">Using Q-Learning and Policy Gradient algorithms on the Knapsack problem<a class="hash-link" href="#using-q-learning-and-policy-gradient-algorithms-on-the-knapsack-problem" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="a-simple-introduction-to-policy-gradient">A simple introduction to Policy Gradient<a class="hash-link" href="#a-simple-introduction-to-policy-gradient" title="Direct link to heading">​</a></h3><p>The basic principle with a policy gradient approach is that, for each state <em>s</em> received as input, our algorithm will provide us a probability distribution for the actions to take, allowing us to know which object our algorithm recommands us to put first in the knapsack.</p><p>In the formula below, 𝜋 gives us this probability. More precisely, 𝜋 gives us the probability to take action <em>a</em> knowing that we currently at state <em>s</em> and given the 𝜃 values of the model parameters, neurons in our case as we use neural networks. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/policy_gradient_equation_2-ed0850ea2e011caa290c188e973a45ca.svg" width="168" height="45" class="img_ev3q"></p><div align="center"></div><br><p>The use of neural networks isn't mandatory here, but very frequent to obtain good results on complex problems. An example of a representation of a simple neural network is given recalled below.  </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/classical_neural_network_image_2-bc3d584b1817a99f9d27d36fd1ab8d75.png" width="692" height="411" class="img_ev3q"></p><div align="center"> Figure 6 - Representation of a simple neural network</div><br><p>Our model architecture can be visualized with this representation. If we dig a bit deeper into the details of our model's architecture, we have:</p><ul><li><p>As input of the neural network, a description of the current state of the system, that is the value and price information for all items, information about the already selected items and about the current and maximum weight limit inside the different bags;</p></li><li><p>Thanks to this information, our model associates to each possible action a probability and we can then select the action that the algorithm recommends us to take first, that is which object should be stored in which knapsack at the current timestep. This is the output of our model;</p></li><li><p>The parameters of this model, the neurons, are updated at the end of each episode, an episode beginning when all the objects are available and ending when the bags are sufficiently full (or all objects have been selected...). The update of the parameters (𝜃) of the model taking place at the end of each episode only and not each time an action is proposed by the model, the approach is called a <strong>Monte Carlo approach</strong>;</p></li><li><p>The updates of the parameters are made in order to maximize the value of the items stored inside the knapsacks and this approach is based on techniques such as <strong>stochastic gradient descent</strong>.</p></li></ul><p>Now that the Policy Gradient has been described, let's see how our algorithms performed on the knapsack problem!</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="evaluating-the-performance-of-the-rl-algorithms">Evaluating the performance of the RL algorithms<a class="hash-link" href="#evaluating-the-performance-of-the-rl-algorithms" title="Direct link to heading">​</a></h3><p>As summarized in figure 7 below, in order to evaluate the performance of the different algorithms, we chose to apply our two RL algorithms (Q-Learning and Policy Gradient) to 3 different environments of increasing difficulty. We trained each algorithm over 400 episodes.</p><p>At the beginning of a new experience, the algorithm had all its coefficients reinitialized. We perform several experiences in order to evaluate how robust is the algorithm. </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/evaluation_rl_algos_2-4e81b4faf5f33e55158b446b50a0d583.png" width="3104" height="1218" class="img_ev3q"></p><div align="center"> Figure 7 - Description of the evaluation process for the different algorithms</div><br><p>Eventually, we evaluated the performance of the algorithms with 3 metrics:</p><ul><li>The mean reward shows how good on average the algorithm is;</li><li>The standard deviation highlights the potential lack of robustness of the algorithm;</li><li>The performance ratio RL vs MILP tells us how close is the RL algorithm to the optimal solution provided by a MILP solver (details on how to obtain such a solution are given in the notebook). </li></ul><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results-with-q-learning-no-neural-networks">Results with Q-Learning (no neural networks)<a class="hash-link" href="#results-with-q-learning-no-neural-networks" title="Direct link to heading">​</a></h3><p>The graphs on the left show that overall our Q-Learning algorithm does indeed improve through training as its reward improved over time. However, we can see that the performance ratio RL vs MILP is very low, meaning that it is far from achieving as good results as what we could get using state-of-the-art MILP solvers.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/performance_q_learning_knapsack_4-9dde740d1774022a4a3d0668e3ae8e78.png" width="3262" height="1697" class="img_ev3q"></p><div align="center"> Figure 8 - Performance of the Q-Learning algorithm</div><br><p>Furthermore, environments of increasing complexity / dimensionality will be much more difficult to handle for Q-Learning, as its Q-value matrix has as number of columns the number of items multiplied by the number of knapsacks and as rows all the possible states which could exist. Increasing only slightly the number of knapsacks or bags will thus quickly make the Q-Learning algorithm unusable.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="results-with-policy-gradient-based-on-neural-networks">Results with Policy Gradient (based on neural networks)<a class="hash-link" href="#results-with-policy-gradient-based-on-neural-networks" title="Direct link to heading">​</a></h3><p>The results of the learning process with Policy Gradient are much better than with Q-learning. At the beginning, due to random initialization of the neural network parameters, the actions are taken at random and the reward is very low, but it quickly improves until reaching a local maximum, not global as it is still lower than the solution obtained with the MILP solver. </p><p>The performance ratio is quite good on the three different environments, reaching approximately 80% for each. The algorithm scales well when the complexity increases.</p><p>We see however that the standard deviation is quite high, which highlights the fact that each time a model is initialized, it can converge to quite different values. It is thus not extremely robust and several initializations are required before finding good results approaching the optimal solution.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/performance_policy_gradient_knapsack_2-dc5ac694619364f3bad0e9927fe54ad1.png" width="2944" height="1674" class="img_ev3q"></p><div align="center"> Figure 9 - Performance of the REINFORCE (policy-gradient approach) algorithm</div><br><p>On the graph below are highlighted some of the limitations we have witnessed with Policy Gradient algorithms such as the REINFORCE algorithm. We have a lack of robustness, having our algorithm sometimes working very well, sometimes leading to a poorer reward.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/robustness_pb_policy_gradient_2-ccd31fb5440a6da8290fa43122747787.png" width="713" height="394" class="img_ev3q"></p><div align="center"> Figure 10 - REINFORCE algorithm (gradient-policy approach) appears as lacking robustness</div><br>For that reason, the hyperparameter tuning is made more complicated. Indeed comparing one combination of hyperparameters with another one isn't enough to be certain about which combination of hyperparameters is the best, because of the high variability of results for fixed hyperparameters.<h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="opening">Opening<a class="hash-link" href="#opening" title="Direct link to heading">​</a></h2><p>For obtaining the results given in this article, we reproduced classical Reinforcement Learning algorithms: the Q-learning algorithm which does not rely on the use of neural networks, and a Policy Gradient algorithm which relies on neural networks. We saw that the latter obtained much better results than the former as we could expect. </p><p>We also built our own multi-knapsack environment file, allowing us to easily modify greatly the complexity of the environment by increasing both the number of available items to put in the knapsacks and the number of knapsacks. </p><p>While creating the environment file, we followed the nomenclature proposed by Open AI Gym for building Reinforcement Learning environments, using the same method names used to define an Open AI Gym environment. The objective was to be able to experiment much more quickly in the future by making use of one of the different Deep Reinforcement Learning libraries (Stable Baselines, TF Agents, Tensorforce…). Indeed, these libraries allow access to many different advanced Deep Reinforcement Learning algorithms already implemented, which can directly be used on new problems if the environment file describing the problem has been built using Open AI Gym nomenclature.</p><p>Another article will be written soon to tell more about how to perform hyperparameter tuning for RL using the hyperparameter optimization framework <a href="https://optuna.org/" target="_blank" rel="noopener noreferrer">Optuna</a> and how to compare and evaluate the efficiency of many different RL algorithms using <a href="https://stable-baselines3.readthedocs.io/en/master/" target="_blank" rel="noopener noreferrer">Stable Baselines</a>!</p>]]></content>
        <author>
            <name>Nathan Rouff</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="Operational Research" term="Operational Research"/>
        <category label="Optimization" term="Optimization"/>
        <category label="Knapsack problem" term="Knapsack problem"/>
        <category label="Deep Reinforcement Learning" term="Deep Reinforcement Learning"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep RL and Optimization applied to Operations Research problem - 1/2 Traditional Optimization techniques]]></title>
        <id>/2022/08/27/traditional_or</id>
        <link href="https://eki-ghazouani.github.io/blog/2022/08/27/traditional_or"/>
        <updated>2022-08-27T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[This article is part of a series of articles which will introduce several optimization techniques, from traditional (yet advanced) Mathematical Optimization solvers and associated packages to Deep Reinforcement Learning algorithms, while tackling a very famous Operations Research problem: the multi-knapsack problem. Here, the focus is on traditional optimization techniques.]]></summary>
        <content type="html"><![CDATA[<p>Let <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">f\colon[a,b]\to\R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mspace nobreak"></span><span class="mspace" style="margin-right:0.1111em"></span><span class="mpunct"></span><span class="mspace" style="margin-right:-0.1667em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mrel">:</span></span><span class="mspace" style="margin-right:0.3333em"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord mathbb">R</span></span></span></span></span> be Riemann integrable. Let <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mspace></mspace><mspace width="0.1111em"></mspace><mo lspace="0em" rspace="0.17em"></mo><mtext> ⁣</mtext><mo lspace="0em" rspace="0em">:</mo><mspace width="0.3333em"></mspace><mo stretchy="false">[</mo><mi>a</mi><mo separator="true">,</mo><mi>b</mi><mo stretchy="false">]</mo><mo>→</mo><mi mathvariant="double-struck">R</mi></mrow><annotation encoding="application/x-tex">F\colon[a,b]\to\R</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mspace nobreak"></span><span class="mspace" style="margin-right:0.1111em"></span><span class="mpunct"></span><span class="mspace" style="margin-right:-0.1667em"></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord"><span class="mrel">:</span></span><span class="mspace" style="margin-right:0.3333em"></span><span class="mopen">[</span><span class="mord mathnormal">a</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">b</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:0.6889em"></span><span class="mord mathbb">R</span></span></span></span></span> be
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>∫</mo><mi>a</mi><mi>x</mi></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">F(x)=\int_{a}^{x} f(t)\,dt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1.2151em;vertical-align:-0.3558em"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0006em">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8593em"><span style="top:-2.3442em;margin-left:-0.1945em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span></span></span></span><span style="top:-3.2579em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3558em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">t</span></span></span></span></span>. Then <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> is continuous, and at all <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> such that
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8889em;vertical-align:-0.1944em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span></span></span></span></span> is continuous at <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span>, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.13889em">F</span></span></span></span></span> is differentiable at <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.4306em"></span><span class="mord mathnormal">x</span></span></span></span></span> with <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">F'(x)=f(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0019em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">F</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7519em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>.</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>I</mi><mo>=</mo><msubsup><mo>∫</mo><mn>0</mn><mrow><mn>2</mn><mi>π</mi></mrow></msubsup><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mtext> </mtext><mi>d</mi><mi>x</mi></mrow><annotation encoding="application/x-tex">I = \int_0^{2\pi} \sin(x)\,dx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em"></span><span class="mord mathnormal" style="margin-right:0.07847em">I</span><span class="mspace" style="margin-right:0.2778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em"></span></span><span class="base"><span class="strut" style="height:2.476em;vertical-align:-0.9119em"></span><span class="mop"><span class="mop op-symbol large-op" style="margin-right:0.44445em;position:relative;top:-0.0011em">∫</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.564em"><span style="top:-1.7881em;margin-left:-0.4445em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span><span style="top:-3.8129em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span><span class="mord mathnormal mtight" style="margin-right:0.03588em">π</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.9119em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em"></span><span class="mop">sin</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.1667em"></span><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span></span></span></span></span></div><p>In this first article is introduced a systematic way to approach and solve optimization problems. Then, the multi-knapsack problem itself is introduced. Then we apply the rules defined before on how to solve optimization problems and obtain the optimal solution to the multi-knapsack problem, formulated as a Mixed Integer problem and using Python-MIP package. Let's now introduce simple steps one can follow to approach optimization problems with optimization solvers.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="main-steps-while-creating-an-optimization-model-to-solve-a-business-problem">Main steps while creating an optimization model to solve a business problem<a class="hash-link" href="#main-steps-while-creating-an-optimization-model-to-solve-a-business-problem" title="Direct link to heading">​</a></h2><p>Once a business problem that could benefit from optimization has been identified, we can define a systematic approach based on 3 steps for solving all kind of optimization problems with optimization solvers. These 3 steps are highlighted in the figure below.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/3_steps_math_modelling_4-679cfdabcdf2ab022e54b439d00a8992.png" width="2670" height="567" class="img_ev3q"></p><div align="center"> Figure 1 : The 3 main steps for solving a business problem through optimization</div><br><p>In more details, these 3 steps are: </p><ol><li><p><strong>Create the conceptual mathematical model</strong> that defines the different variables, constraints, etc. in the business problem. This step consists in writing down on paper the equations that define our problem. </p></li><li><p><strong>Translate the conceptual mathematical model into a computer program</strong>. For most programming languages used for optimization, the computer program will largely resembles the mathematical equations one would write on paper.</p></li><li><p><strong>Solve the mathematical model using a math programming solver</strong>. The solver available for Mathematical Programming (solvers such as GLPK, Gurobi, CPLEX...) relies on very sophisticated algorithms. Important algorithms and ideas used in these solvers are, among many others: simplex method, branch &amp; bound, use of heuristics...</p></li></ol><p>Let's see those 3 steps for the case of the multi-knapsack problem.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="the-multi-knapsack-problem">The multi-knapsack problem<a class="hash-link" href="#the-multi-knapsack-problem" title="Direct link to heading">​</a></h2><p>The objective here is, given a set of <em>n</em> items and a set of <em>m</em> knapsacks, to <strong>maximize</strong> the total value of the items put in the knapsacks without exceeding their capacity.</p><p>Below,  w<sub>i</sub> represents the weight of item i,  p<sub>i</sub> the value of item i while  c<sub>j</sub> represents the capacity of knapsack j.</p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/Knapsack_problem_5-31f9629281a4c78ff57ea1b68c6f753e.png" width="1306" height="1033" class="img_ev3q"></p><div align="center"> Figure 2: Description of the multi-knapsack problem</div><br><p>The multi-knapsack is an extension of the classical knapsack problem where instead of considering only one knapsack, we consider as many as we want. This allows to easily extend the complexity of this problem.</p><p>While the problem is relatively easy to define mathematically, it belongs to the class of NP-hard problems. Without going into the details of what defines NP-hard problems, we can easily see that the complexity of the knapsack problems explodes when the number of knapsacks and items increases. Indeed, we have m<sup>n</sup> available combinations we would need to test should we want to apply a brute-force approach for solving this problem. Just with 10 knapsacks and 80 items, there are 10<sup>80</sup> combinations, which is the estimation of the number of atoms in the universe! And 10 knapsacks and 80 items is still quite limited... Let's now try to create the conceptual mathematical model by defining the problem with equations.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="creating-the-conceptual-mathematical-model">Creating the conceptual mathematical model<a class="hash-link" href="#creating-the-conceptual-mathematical-model" title="Direct link to heading">​</a></h3><p>A quick translation of the multi-knapsack problem with equation can be written as the following: </p><p><img loading="lazy" alt="screenshot-app" src="/assets/images/equations_1-de05b24a1b63925c7e9861db33be2341.svg" width="262" height="161" class="img_ev3q">
<img loading="lazy" alt="screenshot-app" src="/assets/images/equations_3-8e121b52240ee3c6995bc91941f72164.svg" width="201" height="19" class="img_ev3q"></p><p>Now that we managed to translate the problem into a set of equations, let's translate this mathematical model so that it is understood by a computer program. Below, we will make use of the Python package <a href="https://www.python-mip.com/" target="_blank" rel="noopener noreferrer">Python-MIP</a> which is open-source and provides tools for modeling and solving Mixed-Integer Linear Programming Problems (MIP), relying on fast open source solvers.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="translating-the-mathematical-model-into-a-computer-program-with-python-mip">Translating the mathematical model into a computer program with Python-MIP<a class="hash-link" href="#translating-the-mathematical-model-into-a-computer-program-with-python-mip" title="Direct link to heading">​</a></h3><p>Before solving the problem, we have to generate an instance for it (have data defining the problem). To do so, you can use the following code that will generate an instance of this problem with 40 items to store in 5 bags.</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">import pandas as pd</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import numpy as np</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import pickle</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">def data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag):</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data = {}</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    weights = np.random.randint(minimum_weight_item, maximum_weight_item, size = number_items)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    values = np.random.randint(minimum_value_item, maximum_value_item, size = number_items)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['weights'] = weights</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['values'] = values</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['items'] = list(range(len(weights)))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['num_items'] = len(weights)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['bins'] = list(range(number_bags))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    data['bin_capacities'] = np.random.randint(0, max_weight_bag, size = number_bags) + np.int(np.mean(data['weights']))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    return(data)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">number_bags = 5</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">number_items = 40</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">minimum_weight_item = 0</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">maximum_weight_item = 75</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">minimum_value_item = 0</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">maximum_value_item = 75</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">max_weight_bag = 150</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">data = data_generator_knapsack(number_bags, number_items, minimum_weight_item, maximum_weight_item, minimum_value_item, maximum_value_item, max_weight_bag)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Let's now import the package used to have access to the MIP solver, here using the python package Python-MIP:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">from mip import Model, xsum, maximize, BINARY</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Now, we can translate the mathematical model so that it is understood by Python-MIP. </p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">def mip_solve_knapsack(data):</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  model = Model("knapsack")</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  x = [[model.add_var(var_type=BINARY) for i in data['items']] for j in data['bins']]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  model.objective = maximize(xsum((xsum(data['values'][i] * x[j][i] for i in data['items']) for j in data['bins'])))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  for j in data['bins']:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      model += xsum(data['weights'][i] * x[j][i] for i in data['items']) &lt;= data['bin_capacities'][j]</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  # Each item can be in at most one bin</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  for i in data['items']:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">      model += xsum(x[j][i] for j in data['bins']) &lt;= 1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  model.optimize()</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  return(model)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg class="copyButtonIcon_y97N" viewBox="0 0 24 24"><path d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg class="copyButtonSuccessIcon_LjdS" viewBox="0 0 24 24"><path d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Remark how close it is from the original equations! These solvers are very powerful and yet easy to use directly in Python. The code is indeed very close to the original equations. </p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="solving-the-mathematical-model-with-python-mip">Solving the mathematical model with Python-MIP<a class="hash-link" href="#solving-the-mathematical-model-with-python-mip" title="Direct link to heading">​</a></h3><p>Using the <strong>mip_solve_knapsack</strong> function defined in the previous section, we can access to important information regarding the problem, such as the final objective value and the values of x<sub>ij</sub> telling us what were the best combinations of items inside knapsacks.</p><h3 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="some-mathematical-optimization-packages">Some Mathematical Optimization packages<a class="hash-link" href="#some-mathematical-optimization-packages" title="Direct link to heading">​</a></h3><p>In the notebook associated to this article, the package Python-MIP was used. <strong>Python-MIP</strong> is free, but many other packages exist for solving optimization problems on Python (and other languages of course like Julia). For instance <strong>OR-Tools</strong> from Google is a well-recognized free solver, with <a href="https://developers.google.com/optimization/introduction/overview" target="_blank" rel="noopener noreferrer">detailed documentation</a>. </p><p>On the other side, <strong>Gurobi</strong> is a very popular commercial solution for mathematical optimization and its documentation is extremely rich, with quick introductions about <a href="https://www.gurobi.com/resource/modeling-basics/" target="_blank" rel="noopener noreferrer">Mathematical Programming</a>, <a href="https://www.gurobi.com/resource/mip-basics/" target="_blank" rel="noopener noreferrer">Linear Programming</a> and <a href="https://www.gurobi.com/resource/mip-basics/" target="_blank" rel="noopener noreferrer">Mixed-Integer Programming</a>. Importantly, it has a <a href="https://www.gurobi.com/resource/modeling-examples-using-the-gurobi-python-api-in-jupyter-notebook/" target="_blank" rel="noopener noreferrer">large number of modeling examples from all industry fields</a> directly available on Google Colab allowing to better grasp notions of Mathematical Modelling and to improve modeling skills to tackle all kind of optimization problems with Python. This resource can be of use even if one doesn't plan to use this commercial software but rather a free package such as OR-Tools.</p><h2 class="anchor anchorWithHideOnScrollNavbar_WYt5" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">​</a></h2><p>In this article was introduced the multi-knapsack problem, an NP-complete problem, very difficult to solve when taking many items and bags. </p><p>The approach to solve the multi-knapsack problem relied on Python-MIP, a free optimization package using powerful MILP solvers to solve very efficiently all kinds of optimization problems.</p><p>In the next part of this series on the multi-knapsack problem, well studied in the field of Operations Research and at the heart of many real optimization problems, we'll highlight how Deep Reinforcement Learning can be used in order to solve combinatorial optimization problems such as this one. Stay tuned!</p>]]></content>
        <author>
            <name>Nathan Rouff</name>
            <uri>mailto:inno@ekimetrics.com</uri>
        </author>
        <category label="Operational Research" term="Operational Research"/>
        <category label="Optimization" term="Optimization"/>
        <category label="Knapsack problem" term="Knapsack problem"/>
        <category label="Solvers" term="Solvers"/>
    </entry>
</feed>